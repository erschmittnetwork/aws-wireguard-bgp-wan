% =============================================================================
% Document: AWS WireGuard BGP WAN with Automated HA Hub Failover
% Author: Eric Schmitt
% Purpose:
%   Enterprise-style technical documentation of a lab-based WAN reference
%   architecture. Written in structured, RFC-aligned format for clarity,
%   reproducibility, and professional presentation.
%
% Build Notes:
%   - Compiled via standard LaTeX toolchain (Pandoc/pdflatex).
%   - Hyperref used for internal cross-referencing and PDF metadata behavior.
%
% Commenting Philosophy:
%   - Comments are relegated to contextualizing normative claims, design
%     choices, change sensitivities, and verbatim data output.
%   - Content sections are intentionally self-explanatory and not redundantly
%     documented inline.
% =============================================================================

% -----------------------------------------------------------------------------
% Document Class
% -----------------------------------------------------------------------------
% Standard article class selected for portability and predictable layout.
% 11pt provides professional readability without excess pagination.
% -----------------------------------------------------------------------------
\documentclass[11pt]{article}

% -----------------------------------------------------------------------------
% Hyperref Configuration
% -----------------------------------------------------------------------------
% Enables:
%   - Internal section cross-referencing
%   - Clickable TOC entries
%   - Clean PDF open behavior (FitH view)
%
% No custom link coloring applied to preserve formal documentation tone.
% -----------------------------------------------------------------------------
\usepackage{hyperref}

\hypersetup{
  pdfstartpage=1,
  pdfstartview=FitH,
}

% -----------------------------------------------------------------------------
% Title Metadata
% -----------------------------------------------------------------------------
% Title and versioning intentionally reflect production-style document control.
% Copyright statement embedded in author field for formal presentation.
% -----------------------------------------------------------------------------
\title{AWS WireGuard BGP WAN with Automated HA Hub Failover}
\subtitle{Version 1.3}
\author{© 2026 Eric Schmitt. All rights reserved.}

\begin{document}
\maketitle
\thispagestyle{empty}
\newpage

% =============================================================================
% SECTION 1: DOCUMENT CONTROL
%
% Mirrors enterprise and standards-body documentation patterns.
% Establishes traceability, scope boundaries, terminology control, and
% audience assumptions before technical implementation details begin.
% =============================================================================
\section{1. Document Control}
\label{sec:document-control}

\subsection{1.1 Document Metadata}
\label{sec:document-metadata}

\begin{tabular}{|l|l|}
\hline
\textbf{Field} & \textbf{Value} \\
\hline
Document Title & AWS WireGuard BGP WAN with Automated HA Hub Failover \\
\hline
Author & Eric Schmitt \\
\hline
Document Version & 1.3 \\
\hline
Date of Issue & February 10, 2026 \\
\hline
Document Status & Production-Ready (Educational Lab / Proof-of-Concept) \\
\hline
Primary Technologies & WireGuard, BGP (FRR), AWS EC2, EventBridge, Lambda, CloudWatch \\
\hline
Architecture Pattern & Hub-and-Spoke WAN \\
\hline
Cloud Provider & Amazon Web Services (AWS), Region: us-east-1 \\
\hline
\end{tabular}

\subsection{1.2 Change History}
\label{sec:change-history}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Version} & \textbf{Date} & \textbf{Description} & \textbf{Author} \\
\hline
1.0 & 2025-12-01 & Initial professional documentation of lab implementation & Eric Schmitt \\
\hline
1.1 & 2025-12-14 & Validation alignment and terminology precision & Eric Schmitt \\
\hline
1.2 & 2026-01-25 & Raw data review and LaTeX formatting overhaul & Eric Schmitt \\
\hline
1.3 & 2026-02-10 & Finalized document for current lab state & Eric Schmitt \\
\hline
\end{tabular}

\subsection{1.3 Intended Audience}
\label{sec:intended-audience}

This document is intended for technical and technical-adjacent stakeholders:

\begin{itemize}
  \item Network Engineers responsible for WAN routing, encrypted transport, and control-plane design
  \item Cloud Engineers responsible for AWS infrastructure and automation
  \item Platform / Infrastructure Engineers evaluating hybrid WAN reference patterns
  \item Technical Management reviewing architectural viability, resilience, and operational readiness
\end{itemize}

Assumed baseline familiarity:

\begin{itemize}
  \item IP routing fundamentals and troubleshooting concepts
  \item BGP routing concepts (eBGP adjacency, route propagation)
  \item Linux networking (interfaces, routes, forwarding)
  \item AWS EC2 fundamentals (instances, Elastic IPs, security groups, IAM)
\end{itemize}

\subsection{1.4 Terminology and Naming Conventions}
\label{sec:terminology}

\subsubsection{1.4.1 Architectural Terminology}
\label{sec:architectural-terminology}

This document uses hub-and-spoke WAN terminology consistently:

\begin{itemize}
  \item \textbf{Hub}: Central aggregation nodes hosted in AWS EC2, terminating WireGuard tunnels and acting as eBGP transit hubs for this lab scope
  \item \textbf{Spoke}: Remote nodes (VM-based) connecting into the hub via WireGuard and participating in BGP for reachability exchange
\end{itemize}

This terminology is selected to align with common enterprise WAN design language.

\subsubsection{1.4.2 Device Naming Note}
\label{sec:device-naming}

The lab device hostnames reflect a spine/leaf naming convention from an earlier architectural design and are retained verbatim for fidelity to the implementation:

\begin{tabular}{|l|l|}
\hline
\textbf{Logical Role} & \textbf{Hostname Examples} \\
\hline
Hub (Active/Standby Pair) & DC1-SP1, DC1-SP2 \\
\hline
Spokes & SITE1-LF1, SITE2-LF1, SITE3-LF1 \\
\hline
\end{tabular}

Mapping used throughout this document:

\begin{itemize}
  \item References to \textbf{hub} correspond to \texttt{DC1-SPx} devices.
  \item References to \textbf{spoke} correspond to \texttt{SITEx-LF1} devices.
\end{itemize}

\subsection{1.5 Scope and Applicability}
\label{sec:scope}

This document describes a fully functional lab implementation designed to validate an enterprise-style WAN pattern:

\begin{itemize}
  \item Encrypted underlay transport using WireGuard
  \item Dynamic routing overlay using eBGP (FRRouting)
  \item Automated active/standby hub failover and failback using AWS EventBridge and Lambda
  \item Health-based contingency failover using CloudWatch Alarm triggering the same failover Lambda
\end{itemize}

Although small in scale, the implementation is intentionally aligned with enterprise principles such that it can serve as a baseline for a small-to-medium production WAN with minimal modification.

Out of scope for this document:

\begin{itemize}
  \item Active/active hub routing and per-flow distribution across hubs
  \item Multi-region hub deployments
  \item VRF-based segmentation / multi-tenant overlays
  \item SD-WAN orchestration layers and controller-based policy distribution
\end{itemize}

\subsection{1.6 Diagram Reference}
\label{sec:diagram-reference}

A high-level reference architecture diagram is included later in this document (see Section~\ref{sec:reference-architecture}). It illustrates:

\begin{itemize}
  \item Hub-and-spoke topology and addressing
  \item WireGuard underlay termination at the hub pair
  \item BGP overlay reachability exchange for loopback reachability
  \item AWS-controlled Elastic IP reassociation for automated failover and failback
\end{itemize}

% =============================================================================
% SECTION 2: EXECUTIVE SUMMARY
%
% Purpose:
%   Provides a high-level architectural and operational overview of the
%   implemented WAN design prior to deep technical validation and
%   configuration detail in later sections.
%
% Design Intent:
%   - Frame the solution in enterprise networking terminology.
%   - Summarize topology, routing model, and HA mechanisms.
%   - Define operational behaviors validated in the lab.
%
% Audience:
%   Executives, senior engineers, or reviewers who require architectural
%   clarity without immediately engaging in configuration-level detail.
% =============================================================================
\section{2. Executive Summary}
\label{sec:executive-summary}

\subsection{2.1 Overview}
\label{sec:overview}

This document describes the design, implementation, and validation of an AWS-hosted hub-and-spoke WAN architecture using WireGuard for encrypted transport, BGP (FRRouting) for dynamic routing, and AWS-native automation for active/standby high availability.

The environment is implemented as a lab / proof-of-concept but is intentionally designed to align with enterprise-grade architectural principles, including deterministic routing behavior, encrypted transport, automated failure handling, and operational observability. While not feature-complete relative to large-scale production WANs (e.g., active/active hubs, VRFs, or centralized SD-WAN controllers), all architectural decisions reflect realistic operational constraints and failure scenarios commonly encountered in small-to-medium enterprise deployments.

The solution demonstrates that a minimal infrastructure footprint—two cloud-based hubs and multiple remote spokes—can deliver encrypted connectivity, dynamic routing, and automated failover without reliance on proprietary SD-WAN platforms.

\subsection{2.2 Architectural Summary}
\label{sec:architectural-summary}

The WAN follows a hub-and-spoke topology:

\begin{itemize}
  \item Two AWS EC2 instances act as eBGP transit hub nodes (DC1-SP1 and DC1-SP2), configured as an active/standby pair.
  \item Multiple remote spoke nodes (SITE1-LF1, SITE2-LF1, SITE3-LF1) connect to the hubs over WireGuard tunnels.
  \item All routing intelligence is exchanged dynamically using eBGP, with each node advertising a single loopback address representing its logical presence in the WAN.
\end{itemize}

Only one hub is active at any given time. An AWS Elastic IP (EIP) serves as the stable public endpoint for all spokes. Although both hub instances are configured to advertise the same hub loopback prefix, only the hub currently associated with the Elastic IP is reachable. Automated reassociation of this EIP ensures that spoke connectivity is preserved during hub failure or recovery events without requiring any routing changes on the spokes.

\subsection{2.3 High Availability and Automation}
\label{sec:ha-summary}

High availability is achieved using AWS-native event-driven automation rather than traditional network-layer redundancy protocols.

Two independent but complementary mechanisms are implemented:

\begin{itemize}
  \item \textbf{EventBridge-driven failover and failback}:  
  EC2 instance state-change events trigger AWS Lambda functions that move the Elastic IP between hub instances when the primary hub stops or starts.
  \item \textbf{CloudWatch health-based failover}:  
  A CloudWatch alarm monitors EC2 instance health checks. When the primary hub becomes unhealthy (even if still running), the same failover Lambda is triggered to move the Elastic IP to the standby hub.
\end{itemize}

This dual mechanism ensures resilience against both hard failures (instance stoppage) and soft failures (instance running but impaired).

\subsection{2.4 Routing and Reachability Model}
\label{sec:routing-model}

Dynamic routing is handled exclusively via eBGP for overlay reachability:

\begin{itemize}
  \item Each spoke advertises a unique /32 loopback address.
  \item The hub advertises a shared /32 loopback address representing the logical hub service endpoint.
  \item No static routes are used for overlay (inter-site or spoke-to-hub) reachability; all overlay prefixes are learned and installed via BGP.
\end{itemize}

Underlay connectivity relies on directly connected kernel routes associated with WireGuard interfaces, while all spoke-to-spoke and spoke-to-hub communication flows are determined by BGP-learned routes. This design ensures predictable traffic paths, centralized transit through the active hub, and simplified troubleshooting.

\subsection{2.5 Validation Scope}
\label{sec:validation-scope}

The lab validates the following behaviors end-to-end:

\begin{itemize}
  \item Encrypted WireGuard tunnel establishment and maintenance
  \item BGP adjacency formation and route propagation
  \item Spoke-to-spoke and spoke-to-hub reachability via BGP-learned routes
  \item Seamless hub failover with transient packet loss during Elastic IP reassociation
  \item Automatic failback when the primary hub returns to service
\end{itemize}

Observed behavior, command outputs, routing tables, traceroutes, and AWS logs are documented in later sections of this report (see Section~\ref{sec:validation-testing} and Section~\ref{sec:ha-design}).

\subsection{2.6 Diagram Reference}
\label{sec:exec-diagram-reference}

A high-level architecture diagram is included later in this document using Mermaid syntax (see Section~\ref{sec:reference-architecture}). The diagram visually represents:

\begin{itemize}
  \item The hub-and-spoke WAN topology
  \item WireGuard tunnel termination at the hub pair
  \item BGP route advertisement and propagation
  \item AWS-managed Elastic IP failover and failback flow
\end{itemize}

The diagram is intended as a conceptual aid and does not replace the detailed configurations and validation evidence provided elsewhere.

% =============================================================================
% SECTION 3: SCOPE AND OBJECTIVES
%
% Purpose:
%   Formally defines implementation boundaries, exclusions, and technical
%   objectives for the lab environment.
%
% Architectural Role:
%   - Establishes explicit in-scope vs out-of-scope capabilities.
%   - Documents intentional design constraints.
%   - Prevents assumption of unimplemented enterprise features.
%
% Documentation Rationale:
%   This section functions as a control boundary similar to an RFC scope
%   statement. It ensures reviewers evaluate the architecture against
%   declared objectives rather than inferred expectations.
% =============================================================================
\section{3. Scope and Objectives}
\label{sec:scope-objectives}

\subsection{3.1 Scope of Implementation}
\label{sec:scope-implementation}

This document covers the design, deployment, and validation of a hub-and-spoke WAN architecture implemented using open standards and AWS-native services. The scope of the implementation includes:

\begin{itemize}
  \item Encrypted WAN transport using WireGuard
  \item Dynamic routing using eBGP via FRRouting (FRR)
  \item Active/standby hub redundancy using AWS Elastic IP reassociation
  \item Event-driven automation using AWS EventBridge, Lambda, and CloudWatch
  \item End-to-end validation of routing, reachability, and failover behavior
\end{itemize}

The environment is intentionally minimal in size but complete in functionality. It is designed to model a realistic enterprise WAN deployment pattern while remaining small enough to be fully observable and auditable.

\subsection{3.2 Out-of-Scope Items}
\label{sec:out-of-scope}

The following items are explicitly excluded from the scope of this document:

\begin{itemize}
  \item Active/active hub operation or ECMP-based multipath forwarding
  \item Advanced BGP policy controls (route-maps, communities, traffic engineering)
  \item VRF segmentation or multi-tenant routing
  \item Automated WireGuard key rotation
  \item Integration with centralized identity providers or PKI
  \item Performance benchmarking beyond functional validation
\end{itemize}

Where relevant, these topics are discussed at a conceptual level as future enhancements but are not implemented in the current lab (see Section~\ref{sec:design-tradeoffs}).

\subsection{3.3 Design Objectives}
\label{sec:design-objectives}

The primary objectives of this lab are technical rather than commercial. The design is intended to validate architectural soundness, operational feasibility, and automation correctness.

Key objectives include:

\begin{itemize}
  \item Demonstrate a production-aligned WAN architecture using open-source tooling
  \item Validate that WireGuard can function as a routed overlay under BGP control
  \item Achieve deterministic spoke-to-spoke routing via a centralized hub
  \item Implement automated hub failover without requiring reconfiguration of spokes
  \item Ensure that failover behavior is observable, auditable, and repeatable
\end{itemize}

\subsection{3.4 Operational Objectives}
\label{sec:operational-objectives}

From an operational perspective, the lab seeks to demonstrate:

\begin{itemize}
  \item Clear separation between underlay transport and overlay routing
  \item Minimal configuration requirements on spoke nodes
  \item Predictable behavior during hub failure and recovery
  \item Use of cloud-native services for availability rather than protocol-level redundancy
\end{itemize}

Operational simplicity is treated as a first-class design requirement. All mechanisms used for availability and routing are intentionally explicit and observable rather than implicit or opaque.

\subsection{3.5 Intended Audience and Use}
\label{sec:intended-use}

While this document is primarily technical, it is structured to be accessible to multiple audiences:

\begin{itemize}
  \item \textbf{Network Engineers}: Detailed configuration, routing behavior, and validation evidence
  \item \textbf{Cloud Engineers}: AWS service integration, automation logic, and failure handling
  \item \textbf{Technical Leadership}: Architectural rationale, risk trade-offs, and extensibility
\end{itemize}

The document is suitable both as a learning artifact and as a reference baseline for adapting the design to a production WAN environment.

% =============================================================================
% SECTION 4: REFERENCE ARCHITECTURE
%
% Purpose:
%   Defines the canonical architectural model for the lab implementation,
%   including topology, addressing, redundancy, and control-plane separation.
%
% Architectural Significance:
%   This section serves as the normative design baseline against which
%   configuration and validation sections are evaluated.
%
% Rendering Considerations:
%   Includes externally generated diagram artifacts rendered from Mermaid.
%   Diagram source is maintained separately and embedded as a static PNG
%   for deterministic HTML and PDF output.
% =============================================================================
\section{4. Reference Architecture}
\label{sec:reference-architecture}

\subsection{4.1 Architectural Overview}
\label{sec:arch-overview}

The reference architecture implements a hub-and-spoke WAN model using encrypted point-to-point tunnels for transport and BGP for overlay routing. A pair of redundant hub nodes are deployed in AWS, while multiple spoke nodes represent remote sites.

Although the device hostnames retain a \texttt{DC1-SPx} / \texttt{SITEx-LF1} naming convention for continuity with prior lab work, the logical roles are as follows:

\begin{itemize}
  \item \textbf{Hubs}: DC1-SP1 (primary) and DC1-SP2 (standby)
  \item \textbf{Spokes}: SITE1-LF1, SITE2-LF1, SITE3-LF1
\end{itemize}

All inter-site overlay communication is routed via the active hub. Spokes do not establish direct tunnels or routing adjacencies with each other.

\subsection{4.2 High-Level Topology}
\label{sec:high-level-topology}

At a high level, the architecture consists of:

\begin{itemize}
  \item A single logical WAN hub represented by an AWS Elastic IP
  \item Multiple WireGuard tunnels between each spoke and the hub
  \item A BGP overlay advertising loopback reachability across the WAN
  \item AWS-native automation to move the hub identity during failure
\end{itemize}

The topology can be summarized as:

\begin{itemize}
  \item One active hub at any given time
  \item One standby hub with an identical configuration
  \item All spokes connecting to the hub via the same public endpoint
\end{itemize}

\subsection{4.3 Logical Architecture Diagram}
\label{sec:logical-architecture}

Figure~\ref{fig:logical-architecture} illustrates the logical architecture of the deployment.

\begin{quote}
\textit{Diagram implementation note:}\\
The source-of-truth for this diagram is maintained in Mermaid format under
\texttt{diagrams/logical-architecture.mmd}. During CI/CD, Mermaid is rendered to
\texttt{dist/docs/diagrams/logical-architecture.png}, and the document embeds the generated PNG artifact so that both
HTML and PDF outputs are consistent and deterministic.
\end{quote}

% -----------------------------------------------------------------------------
% FIGURE: LOGICAL ARCHITECTURE
%
% Source of truth:
%   diagrams/logical-architecture.mmd
%
% Build behavior:
%   Mermaid diagram is rendered during CI/CD into a static PNG artifact
%   at diagrams/logical-architecture.png.
%
% Rationale:
%   - Ensures deterministic PDF builds (no runtime Mermaid dependency).
%   - Maintains single diagram source for web and PDF outputs.
%
% WARNING:
%   If directory structure changes, update both:
%     1. CI/CD render path
%     2. \includegraphics path below
% -----------------------------------------------------------------------------
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\linewidth]{diagrams/logical-architecture.png}
\caption{Logical architecture: hub-and-spoke WireGuard underlay with BGP overlay and AWS EIP failover.}
\label{fig:logical-architecture}
\end{figure}

\subsection{4.4 Underlay and Overlay Separation}
\label{sec:underlay-overlay}

The architecture explicitly separates concerns between transport and routing:

\begin{itemize}
  \item \textbf{Underlay}: WireGuard tunnels provide encrypted point-to-point connectivity between each spoke and the active hub.
  \item \textbf{Overlay}: BGP distributes reachability information for loopback addresses representing each site and the hub service endpoint.
\end{itemize}

The underlay is intentionally simple and static, while the overlay is dynamic and protocol-driven. This separation allows routing behavior to change without modifying tunnel configuration.

\subsection{4.5 Addressing Model}
\label{sec:addressing-model}

The addressing model uses three distinct logical planes:

\begin{itemize}
  \item \textbf{Underlay Transit}: \texttt{10.100.x.0/30} point-to-point subnets per spoke
  \item \textbf{Overlay Loopbacks}: \texttt{172.16.x.1/32} per hub service and per spoke
  \item \textbf{Public Transport}: AWS public IPs and a shared Elastic IP
\end{itemize}

The hub nodes are configured with identical underlay and loopback IP assignments. Because only one hub is reachable via the Elastic IP at any given time, the logical identity of the hub remains stable during failover events.

\subsection{4.6 Hub Redundancy Model}
\label{sec:hub-redundancy}

Hub redundancy is achieved through an active/standby failover model rather than protocol-level multi-homing.

Key characteristics:

\begin{itemize}
  \item Only one hub is active at a time
  \item Both hubs have identical WireGuard and BGP configurations
  \item An AWS Elastic IP represents the hub's public identity
  \item Automation moves the Elastic IP during failure or recovery
\end{itemize}

From the perspective of the spokes, the hub appears as a single stable endpoint whose availability is maintained externally through cloud-native mechanisms.

\subsection{4.7 Design Rationale}
\label{sec:design-rationale}

This reference architecture was selected to meet the following goals:

\begin{itemize}
  \item Minimize complexity on spoke nodes
  \item Avoid overlapping tunnel policies or routing ambiguity
  \item Leverage cloud-native mechanisms for availability
  \item Maintain clear operational visibility during failure scenarios
\end{itemize}

The resulting design is intentionally conservative but robust, favoring deterministic behavior and operational clarity over maximum throughput or path diversity.

\subsection{4.8 Relationship to Production Deployments}
\label{sec:production-alignment}

While implemented as a lab, this architecture aligns closely with real-world enterprise WAN designs, particularly for:

\begin{itemize}
  \item Small to mid-sized organizations
  \item Cloud-centric hub deployments
  \item Environments prioritizing simplicity and auditability
\end{itemize}

The design can be extended to support additional hubs, active/active routing, or tenant separation, but those enhancements are outside the scope of this reference implementation (see Section~\ref{sec:design-tradeoffs}).

% =============================================================================
% SECTION 5: DETAILED DESIGN
%
% Purpose:
%   Defines the normative technical implementation of the reference
%   architecture, including underlay transport, addressing, identity model,
%   and redundancy mechanics.
%
% Architectural Role:
%   This section translates the conceptual architecture (Section 4) into
%   explicit configuration patterns and deterministic behavior models.
%
% Change Sensitivity:
%   Modifications in this section may impact validation procedures,
%   failover behavior, routing convergence, and security posture.
%   Cross-reference Section 7 (Validation) and Section 6 (HA Design)
%   when altering design elements.
% =============================================================================
\section{5. Detailed Design}
\label{sec:detailed-design}

\subsection{5.1 Design Principles}
\label{sec:design-principles}

The detailed design of this WAN is guided by the following core principles:

\begin{itemize}
  \item \textbf{Deterministic behavior}: All routing and failover outcomes must be predictable and observable.
  \item \textbf{Separation of concerns}: Transport security, routing logic, and high availability mechanisms are independently designed.
  \item \textbf{Minimal spoke complexity}: Spoke nodes are intentionally simple, with a single tunnel and a single routing adjacency.
  \item \textbf{Cloud-native availability}: High availability is implemented using AWS-native primitives rather than in-band routing tricks.
  \item \textbf{Operational transparency}: Failure, recovery, and steady-state behavior must be verifiable using standard tools.
\end{itemize}

These principles reflect common enterprise WAN design constraints while remaining appropriate for a lab-scale proof-of-concept.

\subsection{5.2 Hub-and-Spoke Logical Roles}
\label{sec:logical-roles}

Although the hostnames retain a \texttt{SP} (Spine) / \texttt{LF} (Leaf) naming convention from an earlier architectural design, the logical roles are defined as follows:

\begin{itemize}
  \item \textbf{Hubs}: DC1-SP1 (primary), DC1-SP2 (standby)
  \item \textbf{Spokes}: SITE1-LF1, SITE2-LF1, SITE3-LF1
\end{itemize}

Throughout this document:

\begin{itemize}
  \item The term \textit{hub} refers to the logical eBGP transit hub endpoint represented by the Elastic IP.
  \item The term \textit{spoke} refers to any remote site connecting exclusively to the hub.
\end{itemize}

\subsection{5.3 WireGuard Underlay Design}
\label{sec:wg-underlay-design}

\subsubsection{5.3.1 Interface Model}
\label{sec:wg-interface-model}

Each hub node hosts three independent WireGuard interfaces, one per spoke:

\begin{itemize}
  \item \texttt{wg0}: SITE1-LF1
  \item \texttt{wg1}: SITE2-LF1
  \item \texttt{wg2}: SITE3-LF1
\end{itemize}

Each spoke hosts exactly one WireGuard interface (\texttt{wg0}) connected to the hub.

This model provides:

\begin{itemize}
  \item Clear fault isolation per site
  \item Independent UDP port allocation
  \item Simple troubleshooting and validation
\end{itemize}

\subsubsection{5.3.2 Addressing Scheme}
\label{sec:wg-addressing}

Each hub--spoke tunnel uses a dedicated /30 transit subnet:

% -----------------------------------------------------------------------------
% NOTE: UNDERLAY ADDRESSING TABLE
%
% Each hub instance is configured identically with these interface addresses.
% Only one hub is reachable at a time due to Elastic IP association.
%
% Design Note:
%   Duplicate addressing across hubs is intentional and required to preserve
%   deterministic failover without modifying spoke configuration.
% -----------------------------------------------------------------------------
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Spoke} & \textbf{Hub IP} & \textbf{Spoke IP} \\
\hline
SITE1-LF1 & 10.100.1.1/30 & 10.100.1.2/30 \\
SITE2-LF1 & 10.100.2.1/30 & 10.100.2.2/30 \\
SITE3-LF1 & 10.100.3.1/30 & 10.100.3.2/30 \\
\hline
\end{tabular}
\end{center}

Both hub instances use identical interface IP addressing. At any given time, only the hub associated with the Elastic IP is reachable.

% -----------------------------------------------------------------------------
% 5.3.3 CRYPTOGRAPHIC IDENTITY MODEL
%
% Both hubs intentionally share the same WireGuard keypair to represent a
% single logical cryptographic endpoint. This is required to maintain tunnel
% continuity during Elastic IP reassociation.
%
% SECURITY NOTE:
%   Key reuse across hosts is normally discouraged, but in this design the
%   hubs represent a single logical service identity rather than independent
%   peers.
% -----------------------------------------------------------------------------
\subsubsection{5.3.3 Cryptographic Identity Model}
\label{sec:wg-crypto-identity}

The two hub nodes share the same WireGuard keypair. This ensures:

\begin{itemize}
  \item A single cryptographic identity for the logical hub
  \item Transparent failover at the WireGuard layer
  \item No requirement for spoke reconfiguration during hub transitions
\end{itemize}

Each spoke uses a unique WireGuard keypair.

\subsubsection{5.3.4 Keepalive Strategy}
\label{sec:wg-keepalive}

All hub and spoke configurations use:

\begin{quote}
\texttt{PersistentKeepalive = 25}
\end{quote}

This setting maintains NAT bindings for spoke nodes and accelerates failure detection without excessive control-plane traffic.

% -----------------------------------------------------------------------------
% 5.3.5 WIREGUARD CONFIGURATION ARTIFACTS
%
% Normative Implementation:
%   The following configurations represent the canonical underlay
%   transport configuration used in this deployment.
%
% Symmetry Requirement:
%   Hub configurations (DC1-SP1 and DC1-SP2) are intentionally identical.
%   Any divergence will break deterministic failover behavior.
%
% Redaction Note:
%   Private keys and public IP endpoints are intentionally redacted.
% -----------------------------------------------------------------------------
\subsubsection{5.3.5 WireGuard Configuration Artifacts}
\label{sec:wg-config-artifacts}

\paragraph{Hub WireGuard configuration (DC1-SP1 and DC1-SP2)}

The two hub instances use identical WireGuard configuration files. The following configuration applies to both DC1-SP1 and DC1-SP2.

\begin{verbatim}
# /etc/wireguard/wg0.conf
# DC1-SPx to SITE1-LF1 underlay tunnel interface wg0
[Interface]
Address = 10.100.1.1/30
ListenPort = 51820
PrivateKey = <REDACTED>

# SITE1-LF1
[Peer]
PublicKey = 6IBrUyL5OKE3peUK973ppeNkUkD1FF7hLZ2QsuSAbB4=
AllowedIPs = 10.100.1.2/32, 172.16.1.1/32
Endpoint = <REDACTED: PERSONAL PUBLIC IP>:51820
PersistentKeepalive = 25
\end{verbatim}

\begin{verbatim}
# /etc/wireguard/wg1.conf
# DC1-SPx to SITE2-LF1 underlay tunnel interface wg1
[Interface]
Address = 10.100.2.1/30
ListenPort = 51821
PrivateKey = <REDACTED>

# SITE2-LF1
[Peer]
PublicKey = oNSHCTw4mBKCPEQVo3cmksfqUVbyzmOLpvUeftOvUzE=
AllowedIPs = 10.100.2.2/32, 172.16.2.1/32
Endpoint = <REDACTED: PERSONAL PUBLIC IP>:51820
PersistentKeepalive = 25
\end{verbatim}

\begin{verbatim}
# /etc/wireguard/wg2.conf
# DC1-SPx to SITE3-LF1 underlay tunnel interface wg2
[Interface]
Address = 10.100.3.1/30
ListenPort = 51822
PrivateKey = <REDACTED>

# SITE3-LF1
[Peer]
PublicKey = uNDWDSo9o/Eg0W9OSArxkhUkzfslXLH96ewzOsMIdE4=
AllowedIPs = 10.100.3.2/32, 172.16.3.1/32
Endpoint = <REDACTED: PERSONAL PUBLIC IP>:51820
PersistentKeepalive = 25
\end{verbatim}

\paragraph{Spoke WireGuard configurations}

\begin{verbatim}
# /etc/wireguard/wg0.conf
# SITE1-LF1 to DC1-SPx underlay tunnel interface wg0
[Interface]
Address = 10.100.1.2/30
ListenPort = 51820
PrivateKey = <REDACTED>

# DC1-SPx
[Peer]
PublicKey = YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
AllowedIPs = 10.100.0.0/16, 172.16.0.0/16
Endpoint = <REDACTED: DC1-SPx EIP>:51820
PersistentKeepalive = 25
\end{verbatim}

\begin{verbatim}
# /etc/wireguard/wg0.conf
# SITE2-LF1 to DC1-SPx underlay tunnel interface wg0
[Interface]
Address = 10.100.2.2/30
ListenPort = 51820
PrivateKey = <REDACTED>

# DC1-SPx
[Peer]
PublicKey = YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
AllowedIPs = 10.100.0.0/16, 172.16.0.0/16
Endpoint = <REDACTED: DC1-SPx EIP>:51821
PersistentKeepalive = 25
\end{verbatim}

\begin{verbatim}
# /etc/wireguard/wg0.conf
# SITE3-LF1 to DC1-SPx underlay tunnel interface wg0
[Interface]
Address = 10.100.3.2/30
ListenPort = 51820
PrivateKey = <REDACTED>

# DC1-SPx
[Peer]
PublicKey = YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
AllowedIPs = 10.100.0.0/16, 172.16.0.0/16
Endpoint = <REDACTED: DC1-SPx EIP>:51822
PersistentKeepalive = 25
\end{verbatim}

\subsection{5.4 BGP Overlay Design}
\label{sec:bgp-overlay-design}

\subsubsection{5.4.1 Autonomous System Model}
\label{sec:bgp-as-model}

The WAN uses eBGP with the following AS assignments:

\begin{itemize}
  \item Hub AS: 65000
  \item SITE1-LF1 AS: 65101
  \item SITE2-LF1 AS: 65102
  \item SITE3-LF1 AS: 65103
\end{itemize}

Each spoke forms exactly one eBGP session to the hub.

\subsubsection{5.4.2 Loopback-Based Routing}
\label{sec:bgp-loopbacks}

Each node advertises a single /32 loopback address into BGP:

\begin{itemize}
  \item Hub loopback: 172.16.0.1/32
  \item SITE1-LF1: 172.16.1.1/32
  \item SITE2-LF1: 172.16.2.1/32
  \item SITE3-LF1: 172.16.3.1/32
\end{itemize}

Both hubs advertise the same hub loopback. This is safe in this design because only one hub is reachable via the Elastic IP at any time.

% -----------------------------------------------------------------------------
% 5.4.3 FRROUTING CONFIGURATION ARTIFACTS
%
% Normative Control-Plane Definition:
%   These BGP configurations define the complete overlay routing model.
%
% Determinism Constraint:
%   Hub instances MUST remain configuration-identical, including:
%     - AS number
%     - Router ID
%     - Advertised networks
%
% Policy Model:
%   No route-maps or policy filters are implemented in this lab by design.
% -----------------------------------------------------------------------------
\subsubsection{5.4.3 FRRouting Configuration Artifacts}
\label{sec:bgp-config-artifacts}

\paragraph{Hub BGP configuration (DC1-SP1)}

\begin{verbatim}
router bgp 65000
 bgp router-id 1.1.1.1
 no bgp ebgp-requires-policy
 neighbor 10.100.1.2 remote-as 65101
 neighbor 10.100.1.2 description SITE1-LF1
 neighbor 10.100.2.2 remote-as 65102
 neighbor 10.100.2.2 description SITE2-LF1
 neighbor 10.100.3.2 remote-as 65103
 neighbor 10.100.3.2 description SITE3-LF1
 !
 address-family ipv4 unicast
  network 172.16.0.1/32
 exit-address-family
\end{verbatim}

% -----------------------------------------------------------------------------
% NOTE:
%   DC1-SP2 intentionally uses the same router-id and AS configuration
%   as DC1-SP1. Because only one hub is reachable via the Elastic IP at
%   any given time, no BGP collision occurs.
%
%   This design treats both instances as a single logical router.
% -----------------------------------------------------------------------------
\paragraph{Hub BGP configuration (DC1-SP2)}

DC1-SP2 uses a BGP configuration that is completely identical to DC1-SP1, including router ID and AS number.

\paragraph{Spoke BGP configurations}

\begin{verbatim}
router bgp 65101
 bgp router-id 1.1.1.11
 no bgp ebgp-requires-policy
 neighbor 10.100.1.1 remote-as 65000
 neighbor 10.100.1.1 description DC1-SPx (Hub via EIP)
 !
 address-family ipv4 unicast
  network 172.16.1.1/32
 exit-address-family
\end{verbatim}

\begin{verbatim}
router bgp 65102
 bgp router-id 1.1.1.12
 no bgp ebgp-requires-policy
 neighbor 10.100.2.1 remote-as 65000
 neighbor 10.100.2.1 description DC1-SPx (Hub via EIP)
 !
 address-family ipv4 unicast
  network 172.16.2.1/32
 exit-address-family
\end{verbatim}

\begin{verbatim}
router bgp 65103
 bgp router-id 1.1.1.13
 no bgp ebgp-requires-policy
 neighbor 10.100.3.1 remote-as 65000
 neighbor 10.100.3.1 description DC1-SPx (Hub via EIP)
 !
 address-family ipv4 unicast
  network 172.16.3.1/32
 exit-address-family
\end{verbatim}

\subsection{5.5 Routing Behavior and Traffic Flow}
\label{sec:routing-behavior}

\subsubsection{5.5.1 Hub-to-Spoke Traffic}
\label{sec:hub-to-spoke}

Traffic from the hub to a spoke is routed via the corresponding WireGuard interface based on BGP-learned loopback reachability.

\subsubsection{5.5.2 Spoke-to-Spoke Traffic}
\label{sec:spoke-to-spoke}

Spoke-to-spoke traffic follows the path:

\begin{enumerate}
  \item Source spoke -- hub
  \item Hub performs routing lookup
  \item Hub forwards traffic to destination spoke
\end{enumerate}

No direct spoke-to-spoke tunnels exist.

% -----------------------------------------------------------------------------
% 5.6 HIGH AVAILABILITY DESIGN
%
% Availability Model:
%   High availability is achieved through cloud-level identity reassignment
%   (Elastic IP movement) rather than protocol-level redundancy (e.g., ECMP).
%
% Architectural Intent:
%   Routing protocols remain unaware of redundancy mechanics.
%   Failover is externalized to the infrastructure layer.
% -----------------------------------------------------------------------------
\subsection{5.6 High Availability Design}
\label{sec:ha-design}

\subsubsection{5.6.1 Failure Domains}
\label{sec:failure-domains}

The design explicitly addresses:

\begin{itemize}
  \item Hub instance stoppage or termination
  \item Hub instance health degradation
  \item Planned hub maintenance
\end{itemize}

\subsubsection{5.6.2 Elastic IP as Logical Hub Identity}
\label{sec:eip-identity}

An AWS Elastic IP represents the logical WAN hub endpoint. Exactly one hub holds the EIP at any time, and failover consists solely of reassociating the EIP.

\subsubsection{5.6.3 Automation Triggers}
\label{sec:ha-automation-triggers}

Failover and failback are driven by:

\begin{itemize}
  \item EC2 instance state-change events (EventBridge)
  \item EC2 instance health alarms (CloudWatch)
\end{itemize}

Both invoke Lambda functions responsible for EIP reassociation.

% -----------------------------------------------------------------------------
% 5.7 SECURITY DESIGN CONSIDERATIONS
%
% Scope Clarification:
%   Security controls are limited to transport encryption and basic
%   network-level access restriction.
%
% Explicit Exclusion:
%   Automated key rotation, centralized PKI integration, and host
%   hardening baselines are outside the scope of this lab.
% -----------------------------------------------------------------------------
\subsection{5.7 Security Design Considerations}
\label{sec:security-design}

\subsubsection{5.7.1 Network-Level Controls}
\label{sec:network-controls}

AWS Security Groups restrict inbound WireGuard UDP traffic and SSH access to trusted sources.

\subsubsection{5.7.2 Host-Level Controls}
\label{sec:host-controls}

Host firewalls are kept permissive in this lab to avoid interfering with routing and tunnel behavior.

\subsubsection{5.7.3 Cryptographic Scope}
\label{sec:crypto-scope}

WireGuard provides transport-layer encryption. Key rotation is manual and explicitly out of scope.

% -----------------------------------------------------------------------------
% 5.8 DESIGN LIMITATIONS
%
% This subsection explicitly documents architectural constraints and
% non-goals to prevent overextension of the reference implementation.
%
% These constraints are architectural decisions, not defects..
% -----------------------------------------------------------------------------
\subsection{5.8 Design Limitations}
\label{sec:design-limitations}

\begin{itemize}
  \item Single active hub at any time
  \item No traffic load sharing
  \item No tenant or VRF separation
\end{itemize}

\subsection{5.9 Transition to Validation}
\label{sec:transition-validation}

The design described in this section is validated in:

\begin{itemize}
  \item Section~\ref{sec:validation-testing}
  \item Section~\ref{sec:operations-runbook}
\end{itemize}

All observed behavior matches the intended design outcomes.

% =============================================================================
% SECTION 6: VALIDATION & TESTING
%
% Purpose:
%   Provides empirical verification of the implemented architecture,
%   including underlay integrity, overlay convergence, and failover behavior.
%
% Architectural Role:
%   Demonstrates that the normative design defined in Section 5 operates
%   deterministically under steady-state and transition conditions.
%   All validation artifacts are derived from live system output.
%
% Validation Scope:
%   - WireGuard underlay state and peer health
%   - Routing table correctness
%   - BGP-learned prefix installation
%   - Behavioral consistency during hub role transitions
%
% Change Sensitivity:
%   Any modification to addressing, AllowedIPs definitions, BGP policy,
%   or HA automation may invalidate the validation evidence captured here.
%   Re-run all verification steps after altering design elements.
% =============================================================================
\section{6. Validation \& Testing}
\label{sec:validation-testing}

% -----------------------------------------------------------------------------
% 6.1 WIREGUARD UNDERLAY VALIDATION
%
% This subsection validates the encrypted transport plane independent
% of overlay routing semantics.
%
% Verification Criteria:
%   - All WireGuard interfaces are operational (UP/LOWER_UP).
%   - Each hub maintains one interface per spoke.
%   - Each spoke maintains a single interface toward the active hub EIP.
%   - No static overlay routes are present in the underlay context.
%
% Underlay validation must succeed prior to evaluating BGP convergence
% or overlay reachability.
% -----------------------------------------------------------------------------
\subsection{6.1 WireGuard Underlay Validation}
\label{sec:wg-underlay-validation}

This section validates the encrypted underlay between the AWS hub instances (DC1-SP1, DC1-SP2) and the three site spokes (SITE1-LF1, SITE2-LF1, SITE3-LF1). The goals of this validation are:

\begin{itemize}
  \item Confirm that all WireGuard interfaces are up with the expected addressing.
  \item Verify that each hub has one point-to-point underlay interface per site.
  \item Verify that each spoke has a single underlay interface towards the active hub Elastic IP.
  \item Confirm that underlay routes are correctly installed and that no static overlay routes are present.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.1.1 HUB UNDERLAY INTERFACES
%
% Both hubs are expected to present identical underlay interface
% topology and addressing. Only Elastic IP ownership determines
% active versus standby state.
%
% Validation confirms:
%   - Three point-to-point interfaces per hub (wg0, wg1, wg2).
%   - Correct /30 addressing per site.
%   - Peer AllowedIPs match per-site loopback ownership.
%   - Interface and routing state remain consistent during hub transition.
%
% Behavioral parity between hubs is a prerequisite for deterministic
% failover.
% -----------------------------------------------------------------------------
\subsubsection{6.1.1 Hub Underlay Interfaces}
\label{sec:hub-underlay-interfaces}

\paragraph{DC1-SP1 (Active Hub)}

\paragraph{WireGuard session state}

\begin{verbatim}
sudo wg show
interface: wg0
  public key: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  private key: (hidden)
  listening port: 51820

peer: 6IBrUyL5OKE3peUK973ppeNkUkD1FF7hLZ2QsuSAbB4=
  endpoint: <REDACTED: PERSONAL PUBLIC IP>:42294
  allowed ips: 10.100.1.2/32, 172.16.1.1/32
  latest handshake: 33 seconds ago
  transfer: 79.54 KiB received, 10.96 MiB sent
  persistent keepalive: every 25 seconds

interface: wg1
  public key: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  private key: (hidden)
  listening port: 51821

peer: oNSHCTw4mBKCPEQVo3cmksfqUVbyzmOLpvUeftOvUzE=
  endpoint: <REDACTED: PERSONAL PUBLIC IP>:45588
  allowed ips: 10.100.2.2/32, 172.16.2.1/32
  latest handshake: 1 minute, 56 seconds ago
  transfer: 31.82 KiB received, 11.13 MiB sent
  persistent keepalive: every 25 seconds

interface: wg2
  public key: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  private key: (hidden)
  listening port: 51822

peer: uNDWDSo9o/Eg0W9OSArxkhUkzfslXLH96ewzOsMIdE4=
  endpoint: <REDACTED: PERSONAL PUBLIC IP>:39958
  allowed ips: 10.100.3.2/32, 172.16.3.1/32
  latest handshake: 1 minute, 3 seconds ago
  transfer: 31.79 KiB received, 30.30 KiB sent
  persistent keepalive: every 25 seconds
\end{verbatim}

\paragraph{Interface addressing}

\begin{verbatim}
ip addr show wg0
3: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 8921 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.1.1/30 scope global wg0
       valid_lft forever preferred_lft forever
\end{verbatim}

\begin{verbatim}
ip addr show wg1
4: wg1: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 8921 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.2.1/30 scope global wg1
       valid_lft forever preferred_lft forever
\end{verbatim}

\begin{verbatim}
ip addr show wg2
5: wg2: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 8921 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.3.1/30 scope global wg2
       valid_lft forever preferred_lft forever
\end{verbatim}

\paragraph{Routing table}

\begin{verbatim}
ip route
default via 172.31.64.1 dev ens5 proto dhcp src 172.31.77.50 metric 100
10.100.1.0/30 dev wg0 proto kernel scope link src 10.100.1.1
10.100.2.0/30 dev wg1 proto kernel scope link src 10.100.2.1
10.100.3.0/30 dev wg2 proto kernel scope link src 10.100.3.1
172.16.1.1 dev wg0 scope link
172.16.2.1 dev wg1 scope link
172.16.3.1 dev wg2 scope link
172.31.0.2 via 172.31.64.1 dev ens5 proto dhcp src 172.31.77.50 metric 100
172.31.64.0/20 dev ens5 proto kernel scope link src 172.31.77.50 metric 100
172.31.64.1 dev ens5 proto dhcp scope link src 172.31.77.50 metric 100
\end{verbatim}

\paragraph{DC1-SP2 (Standby Hub)}

The following output is captured while DC1-SP1 is stopped and DC1-SP2 is associated with the Elastic IP.

\paragraph{WireGuard session state}

\begin{verbatim}
sudo wg show
interface: wg0
  public key: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  private key: (hidden)
  listening port: 51820

peer: 6IBrUyL5OKE3peUK973ppeNkUkD1FF7hLZ2QsuSAbB4=
  endpoint: <REDACTED: PERSONAL PUBLIC IP>:42294
  allowed ips: 10.100.1.2/32, 172.16.1.1/32
  latest handshake: 2 minutes, 14 seconds ago
  transfer: 46.36 KiB received, 13.59 MiB sent
  persistent keepalive: every 25 seconds

interface: wg1
  public key: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  private key: (hidden)
  listening port: 51821

peer: oNSHCTw4mBKCPEQVo3cmksfqUVbyzmOLpvUeftOvUzE=
  endpoint: <REDACTED: PERSONAL PUBLIC IP>:45588
  allowed ips: 10.100.2.2/32, 172.16.2.1/32
  latest handshake: 11 seconds ago
  transfer: 26.54 KiB received, 13.59 MiB sent
  persistent keepalive: every 25 seconds

interface: wg2
  public key: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  private key: (hidden)
  listening port: 51822

peer: uNDWDSo9o/Eg0W9OSArxkhUkzfslXLH96ewzOsMIdE4=
  endpoint: <REDACTED: PERSONAL PUBLIC IP>:39958
  allowed ips: 10.100.3.2/32, 172.16.3.1/32
  latest handshake: 2 minutes, 32 seconds ago
  transfer: 7.55 KiB received, 13.66 MiB sent
  persistent keepalive: every 25 seconds
\end{verbatim}

\paragraph{Interface addressing}

\begin{verbatim}
ip addr show wg0
9: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 8921 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.1.1/30 scope global wg0
       valid_lft forever preferred_lft forever
\end{verbatim}

\begin{verbatim}
ip addr show wg1
10: wg1: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 8921 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.2.1/30 scope global wg1
       valid_lft forever preferred_lft forever
\end{verbatim}

\begin{verbatim}
ip addr show wg2
11: wg2: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 8921 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.3.1/30 scope global wg2
       valid_lft forever preferred_lft forever
\end{verbatim}

\paragraph{Routing table}

\begin{verbatim}
ip route
default via 172.31.64.1 dev ens5
default via 172.31.64.1 dev ens5 proto dhcp src 172.31.67.250 metric 100
10.100.1.0/30 dev wg0 proto kernel scope link src 10.100.1.1
10.100.2.0/30 dev wg1 proto kernel scope link src 10.100.2.1
10.100.3.0/30 dev wg2 proto kernel scope link src 10.100.3.1
172.16.1.1 dev wg0 scope link
172.16.2.1 dev wg1 scope link
172.16.3.1 dev wg2 scope link
172.31.0.2 via 172.31.64.1 dev ens5 proto dhcp src 172.31.67.250 metric 100
172.31.64.0/20 dev ens5 proto kernel scope link src 172.31.67.250
172.31.64.1 dev ens5 proto dhcp scope link src 172.31.67.250 metric 100
\end{verbatim}

% -----------------------------------------------------------------------------
% 6.1.2 SPOKE UNDERLAY INTERFACES
%
% Spokes are intentionally single-homed at the underlay layer.
% Each spoke maintains exactly one WireGuard interface toward the
% hub group via the shared Elastic IP.
%
% Validation confirms:
%   - Correct /30 addressing per spoke.
%   - AllowedIPs include full overlay prefix space.
%   - Overlay prefixes are learned via BGP, not statically defined.
%
% This enforces simplicity at the edge and removes hub-selection logic
% from spoke configuration.
% -----------------------------------------------------------------------------
\subsubsection{6.1.2 Spoke Underlay Interfaces}
\label{sec:spoke-underlay-interfaces}

\paragraph{SITE1-LF1}

\paragraph{WireGuard session state}

\begin{verbatim}
sudo wg show
interface: wg0
  public key: 6IBrUyL5OKE3peUK973ppeNkUkD1FF7hLZ2QsuSAbB4=
  private key: (hidden)
  listening port: 51820

peer: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  endpoint: <REDACTED: DC1-SPx EIP>:51820
  allowed ips: 10.100.0.0/16, 172.16.0.0/16
  latest handshake: 35 seconds ago
  transfer: 77.21 KiB received, 74.56 KiB sent
  persistent keepalive: every 25 seconds
\end{verbatim}

\paragraph{Interface addressing}

\begin{verbatim}
ip addr show wg0
4: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.1.2/30 scope global wg0
       valid_lft forever preferred_lft forever
\end{verbatim}

\paragraph{Routing table}

\begin{verbatim}
ip route
default via 192.168.195.2 dev ens33 proto dhcp src 192.168.195.136 metric 100
10.100.0.0/16 dev wg0 scope link
10.100.1.0/30 dev wg0 proto kernel scope link src 10.100.1.2
172.16.0.0/16 dev wg0 scope link
172.16.0.1 nhid 18 via 10.100.1.1 dev wg0 proto bgp metric 20
172.16.2.1 nhid 18 via 10.100.1.1 dev wg0 proto bgp metric 20
172.16.3.1 nhid 18 via 10.100.1.1 dev wg0 proto bgp metric 20
192.168.5.0/24 dev ens38 proto kernel scope link src 192.168.5.130
192.168.195.0/24 dev ens33 proto kernel scope link src 192.168.195.136 metric 100
192.168.195.2 dev ens33 proto dhcp scope link src 192.168.195.136
\end{verbatim}

\paragraph{SITE2-LF1}

\paragraph{WireGuard session state}

\begin{verbatim}
sudo wg show
interface: wg0
  public key: oNSHCTw4mBKCPEQVo3cmksfqUVbyzmOLpvUeftOvUzE=
  private key: (hidden)
  listening port: 51820

peer: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  endpoint: <REDACTED: DC1-SPx EIP>:51821
  allowed ips: 10.100.0.0/16, 172.16.0.0/16
  latest handshake: 53 seconds ago
  transfer: 43.03 KiB received, 33.08 KiB sent
  persistent keepalive: every 25 seconds
\end{verbatim}

\paragraph{Interface addressing}

\begin{verbatim}
ip addr show wg0
4: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.2.2/30 scope global wg0
       valid_lft forever preferred_lft forever
\end{verbatim}

\paragraph{Routing table}

\begin{verbatim}
ip route
default via 192.168.195.2 dev ens33
default via 192.168.195.2 dev ens33 proto dhcp src 192.168.195.135 metric 100
10.100.0.0/16 dev wg0 scope link
10.100.2.0/30 dev wg0 proto kernel scope link src 10.100.2.2
172.16.0.0/16 dev wg0 scope link
172.16.0.1 nhid 20 via 10.100.2.1 dev wg0 proto bgp metric 20
172.16.1.1 nhid 20 via 10.100.2.1 dev wg0 proto bgp metric 20
172.16.3.1 nhid 20 via 10.100.2.1 dev wg0 proto bgp metric 20
192.168.5.0/24 dev ens38 proto kernel scope link src 192.168.5.128
192.168.195.0/24 dev ens33 proto kernel scope link src 192.168.195.135 metric 100
192.168.195.2 dev ens33 proto dhcp scope link src 192.168.195.135
\end{verbatim}

\paragraph{SITE3-LF1}

\paragraph{WireGuard session state}

\begin{verbatim}
sudo wg show
interface: wg0
  public key: uNDWDSo9o/Eg0W9OSArxkhUkzfslXLH96ewzOsMIdE4=
  private key: (hidden)
  listening port: 51820

peer: YREXmIKOLEvUp8A9gesbe/tM/jd160iv2oXWvizwTSI=
  endpoint: <REDACTED: DC1-SPx EIP>:51822
  allowed ips: 10.100.0.0/16, 172.16.0.0/16
  latest handshake: 2 minutes, 46 seconds ago
  transfer: 31.02 KiB received, 33.71 KiB sent
  persistent keepalive: every 25 seconds
\end{verbatim}

\paragraph{Interface addressing}

\begin{verbatim}
ip addr show wg0
4: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN group
        default qlen 1000
    link/none
    inet 10.100.3.2/30 scope global wg0
       valid_lft forever preferred_lft forever
\end{verbatim}

\paragraph{Routing table}

\begin{verbatim}
ip route
default via 192.168.195.2 dev ens33
10.100.0.0/16 dev wg0 scope link
10.100.3.0/30 dev wg0 proto kernel scope link src 10.100.3.2
172.16.0.0/16 dev wg0 scope link
172.16.0.1 nhid 20 via 10.100.3.1 dev wg0 proto bgp metric 20
172.16.1.1 nhid 20 via 10.100.3.1 dev wg0 proto bgp metric 20
172.16.2.1 nhid 20 via 10.100.3.1 dev wg0 proto bgp metric 20
192.168.5.0/24 dev ens38 proto kernel scope link src 192.168.5.129
192.168.195.0/24 dev ens33 proto kernel scope link src 192.168.195.134
\end{verbatim}

% -----------------------------------------------------------------------------
% 6.1.3 UNDERLAY VALIDATION SUMMARY
%
% The encrypted transport layer is confirmed to be:
%   - Fully established across all sites.
%   - Consistent between active and standby hubs.
%   - Free of unintended static overlay routes.
%
% Successful underlay validation establishes the foundation required
% for overlay routing and failover behavior analysis in subsequent
% subsections.
% -----------------------------------------------------------------------------
\subsubsection{6.1.3 Underlay Validation Summary}
\label{sec:underlay-validation-summary}

\begin{itemize}
  \item Each hub maintains three point-to-point WireGuard interfaces, one per site.
  \item Each spoke maintains a single WireGuard interface towards the hub group.
  \item All peers show recent handshakes and non-zero traffic counters.
  \item Underlay routing tables contain only kernel-installed connected routes and BGP-installed overlay routes.
  \item Underlay behavior is identical during hub failover.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.2 BGP OVERLAY VALIDATION
%
% This subsection validates control-plane operation of the eBGP overlay
% transported across the WireGuard underlay.
%
% Validation confirms:
%   - One eBGP adjacency per spoke toward 10.100.X.1.
%   - Hub instances learn all spoke loopbacks.
%   - Spokes learn all remote site loopbacks via the hub.
%   - Overlay reachability is sourced exclusively from BGP.
%
% These checks verify that inter-site routing behavior aligns with the
% architectural model validated in Section 6.1.
% -----------------------------------------------------------------------------
\subsection{6.2 BGP Overlay Validation}
\label{sec:bgp-overlay-validation}

This section validates the BGP overlay used to carry site loopback reachability across the WireGuard underlay. The objectives are:

\begin{itemize}
  \item Confirm that every spoke forms a single eBGP session to the hub group at 10.100.X.1.
  \item Confirm that both hub instances (DC1-SP1 and DC1-SP2) learn all spoke loopbacks via BGP.
  \item Confirm that each spoke learns all other sites' loopbacks via the hub.
  \item Verify that overlay reachability is learned via BGP and that no static overlay routing configuration exists.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.2.1 HUB BGP VIEW -- DC1-SP1 (ACTIVE)
%
% This subsection validates hub control-plane behavior while the instance
% is holding the Elastic IP and serving as active overlay transit node.
%
% Verification Criteria:
%   - Exactly one eBGP session per spoke.
%   - One prefix received from each spoke (/32 loopback).
%   - Hub loopback locally originated.
%   - All spoke loopbacks present in BGP RIB.
%   - eBGP administrative distance (20) observed in FRR route view.
%
% The hub must function strictly as transit AS 65000 with no unintended
% route reflection or iBGP semantics.
% -----------------------------------------------------------------------------
\subsubsection{6.2.1 Hub BGP View -- DC1-SP1 (Active)}
\label{sec:bgp-overlay-hub-dc1-sp1}

\paragraph{DC1-SP1 -- BGP Session Summary}

\paragraph{BGP neighbor status}

\begin{verbatim}
sudo vtysh -c "show ip bgp summary"
\end{verbatim}

\begin{verbatim}
IPv4 Unicast Summary (VRF default):
BGP router identifier 1.1.1.1, local AS number 65000 vrf-id 0
BGP table version 10
RIB entries 7, using 1288 bytes of memory
Peers 3, using 2169 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   
        PfxSnt Desc
10.100.1.2      4      65101        18        16        0    0    0 00:00:21            1
        4 SITE1-LF1
10.100.2.2      4      65102        17        15        0    0    0 00:00:18            1
        4 SITE2-LF1
10.100.3.2      4      65103        19        17        0    0    0 00:00:16            1
        4 SITE3-LF1

Total number of neighbors 3
\end{verbatim}

DC1-SP1 maintains exactly one eBGP session per site, each over its corresponding underlay /30:

\begin{itemize}
  \item SITE1-LF1: 10.100.1.2 (AS 65101)
  \item SITE2-LF1: 10.100.2.2 (AS 65102)
  \item SITE3-LF1: 10.100.3.2 (AS 65103)
\end{itemize}

Each neighbor is in the Established state (State/PfxRcd = 1), with one prefix received from each site (the site loopback). DC1-SP1 advertises four prefixes (the hub loopback plus three site loopbacks) back to each neighbor.

\paragraph{BGP table}

\begin{verbatim}
sudo vtysh -c "show ip bgp"
\end{verbatim}

\begin{verbatim}
BGP table version is 10, local router ID is 1.1.1.1, vrf id 0
Default local pref 100, local AS 65000
Status codes:  s suppressed, d damped, h history, * valid, > best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, < announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete

   Network          Next Hop            Metric LocPrf Weight Path
*> 172.16.0.1/32    0.0.0.0                  0         32768 i
*> 172.16.1.1/32    10.100.1.2               0             0 65101 i
*> 172.16.2.1/32    10.100.2.2               0             0 65102 i
*> 172.16.3.1/32    10.100.3.2               0             0 65103 i

Displayed  4 routes and 4 total paths
\end{verbatim}

Overlay reachability from the hub’s control-plane perspective:

\begin{itemize}
  \item 172.16.0.1/32 is the hub loopback (locally originated via \texttt{network 172.16.0.1/32}).
  \item 172.16.1.1/32 is learned from SITE1-LF1 (AS 65101).
  \item 172.16.2.1/32 is learned from SITE2-LF1 (AS 65102).
  \item 172.16.3.1/32 is learned from SITE3-LF1 (AS 65103).
\end{itemize}

\paragraph{RIB installation (FRR view)}

\begin{verbatim}
sudo vtysh -c "show ip route bgp"
\end{verbatim}

\begin{verbatim}
B   172.16.1.1/32 [20/0] via 10.100.1.2, wg0, weight 1, 00:01:10
B   172.16.2.1/32 [20/0] via 10.100.2.2, wg1, weight 1, 00:01:07
B   172.16.3.1/32 [20/0] via 10.100.3.2, wg2, weight 1, 00:01:05
\end{verbatim}

This output confirms that all spoke loopbacks are present in FRR’s BGP RIB with eBGP administrative distance. As shown in Section~\ref{sec:wg-underlay-validation}, the hub kernel routing table may also contain routes to these prefixes via WireGuard \texttt{AllowedIPs}; therefore, kernel route entries alone are not used here to attribute route origin.

% -----------------------------------------------------------------------------
% 6.2.2 HUB BGP VIEW -- DC1-SP2 (STANDBY)
%
% Validates control-plane equivalence of the standby hub instance.
%
% Architectural Requirement:
%   Both hubs operate with identical BGP configuration, router ID,
%   and AS number. Elastic IP ownership determines forwarding role,
%   not routing configuration.
%
% Verification Criteria:
%   - Same number of neighbors as active hub.
%   - Identical prefix inventory in BGP RIB.
%   - Identical next-hop resolution semantics.
%
% Control-plane symmetry ensures failover does not require routing
% reconvergence beyond session re-establishment.
% -----------------------------------------------------------------------------
\subsubsection{6.2.2 Hub BGP View -- DC1-SP2 (Standby)}
\label{sec:bgp-overlay-hub-dc1-sp2}

\paragraph{DC1-SP2 -- BGP Session Summary (Standby Hub)}

The standby hub (DC1-SP2) runs an identical BGP configuration and uses the same BGP router ID (1.1.1.1) and AS (65000) as DC1-SP1. The following outputs are captured while DC1-SP2 is holding the Elastic IP.

\paragraph{BGP neighbor status}

\begin{verbatim}
sudo vtysh -c "show ip bgp summary"
\end{verbatim}

\begin{verbatim}
IPv4 Unicast Summary (VRF default):
BGP router identifier 1.1.1.1, local AS number 65000 vrf-id 0
BGP table version 14
RIB entries 7, using 1288 bytes of memory
Peers 3, using 2169 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   
        PfxSnt Desc
10.100.1.2      4      65101       181       174        0    0    0 00:25:09            1
        4 SITE1-LF1
10.100.2.2      4      65102       118       115        0    0    0 00:25:22            1
        4 SITE2-LF1
10.100.3.2      4      65103        46        39        0    0    0 00:24:58            1
        4 SITE3-LF1

Total number of neighbors 3
\end{verbatim}

DC1-SP2 forms the same three eBGP adjacencies and learns the same site loopbacks as DC1-SP1.

\paragraph{BGP table}

\begin{verbatim}
sudo vtysh -c "show ip bgp"
\end{verbatim}

\begin{verbatim}
BGP table version is 14, local router ID is 1.1.1.1, vrf id 0
Default local pref 100, local AS 65000

   Network          Next Hop            Metric LocPrf Weight Path
*> 172.16.0.1/32    0.0.0.0                  0         32768 i
*> 172.16.1.1/32    10.100.1.2               0             0 65101 i
*> 172.16.2.1/32    10.100.2.2               0             0 65102 i
*> 172.16.3.1/32    10.100.3.2               0             0 65103 i

Displayed  4 routes and 4 total paths
\end{verbatim}

\paragraph{RIB installation (FRR view)}

\begin{verbatim}
sudo vtysh -c "show ip route bgp"
\end{verbatim}

\begin{verbatim}
B   172.16.1.1/32 [20/0] via 10.100.1.2, wg0, weight 1, 00:57:35
B   172.16.2.1/32 [20/0] via 10.100.2.2, wg1, weight 1, 00:57:48
B   172.16.3.1/32 [20/0] via 10.100.3.2, wg2, weight 1, 00:57:24
\end{verbatim}

DC1-SP2’s BGP control-plane view is identical to DC1-SP1’s, ensuring that hub failover does not require any routing changes on the spokes.

% -----------------------------------------------------------------------------
% 6.2.3 SPOKE BGP VIEW -- SITE1-LF1
%
% Validates spoke overlay behavior with single-homed eBGP adjacency
% toward the hub group.
%
% Verification Criteria:
%   - Exactly one neighbor in AS 65000.
%   - Hub loopback learned via eBGP.
%   - Remote site loopbacks learned with AS path:
%         65000 6510X
%   - Local loopback originated locally (weight 32768).
%
% Confirms hub-only transit model and absence of direct spoke-to-spoke
% peering.
% -----------------------------------------------------------------------------
\subsubsection{6.2.3 Spoke BGP View -- SITE1-LF1}
\label{sec:bgp-overlay-spoke-site1}

\paragraph{SITE1-LF1 -- BGP Overlay View}

\paragraph{BGP neighbor status}

\begin{verbatim}
sudo vtysh -c "show ip bgp summary"
\end{verbatim}

\begin{verbatim}
IPv4 Unicast Summary (VRF default):
BGP router identifier 1.1.1.11, local AS number 65101 vrf-id 0
BGP table version 6
RIB entries 7, using 1288 bytes of memory
Peers 1, using 723 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd
        PfxSnt Desc
10.100.1.1      4      65000        15        15        0    0    0 00:06:18            3
        4 DC1-SP1

Total number of neighbors 1
\end{verbatim}

\paragraph{BGP table}

\begin{verbatim}
sudo vtysh -c "show ip bgp"
\end{verbatim}

\begin{verbatim}
   Network          Next Hop            Metric LocPrf Weight Path
*> 172.16.0.1/32    10.100.1.1               0             0 65000 i
*> 172.16.1.1/32    0.0.0.0                  0         32768 i
*> 172.16.2.1/32    10.100.1.1                             0 65000 65102 i
*> 172.16.3.1/32    10.100.1.1                             0 65000 65103 i

Displayed  4 routes and 4 total paths
\end{verbatim}

\paragraph{RIB installation}

\begin{verbatim}
sudo vtysh -c "show ip route bgp"
\end{verbatim}

\begin{verbatim}
B>* 172.16.0.1/32 [20/0] via 10.100.1.1, wg0, weight 1, 00:06:33
B>* 172.16.2.1/32 [20/0] via 10.100.1.1, wg0, weight 1, 00:06:31
B>* 172.16.3.1/32 [20/0] via 10.100.1.1, wg0, weight 1, 00:06:28
\end{verbatim}

On the spokes, overlay routes are installed in the kernel routing table as \texttt{proto bgp}, as shown in Section~\ref{sec:wg-underlay-validation}.

% -----------------------------------------------------------------------------
% 6.2.4 SPOKE BGP VIEW -- SITE2-LF1
%
% Confirms identical single-homed overlay behavior for SITE2-LF1.
%
% Validation ensures:
%   - One eBGP session toward hub.
%   - Hub loopback and remote site loopbacks learned dynamically.
%   - No alternate transit paths.
%
% Behavioral uniformity across spokes is required for deterministic
% scaling characteristics.
% -----------------------------------------------------------------------------
\subsubsection{6.2.4 Spoke BGP View -- SITE2-LF1}
\label{sec:bgp-overlay-spoke-site2}

\paragraph{SITE2-LF1 -- BGP Overlay View}

\paragraph{BGP neighbor status}

\begin{verbatim}
sudo vtysh -c "show ip bgp summary"
\end{verbatim}

\begin{verbatim}
IPv4 Unicast Summary (VRF default):
BGP router identifier 1.1.1.12, local AS number 65102 vrf-id 0
BGP table version 4
RIB entries 7, using 1288 bytes of memory
Peers 1, using 723 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd
        PfxSnt Desc
10.100.2.1      4      65000        14        14        0    0    0 00:07:08            3
        4 DC1-SP1

Total number of neighbors 1
\end{verbatim}

\paragraph{BGP table}

\begin{verbatim}
sudo vtysh -c "show ip bgp"
\end{verbatim}

\begin{verbatim}
   Network          Next Hop            Metric LocPrf Weight Path
*> 172.16.0.1/32    10.100.2.1               0             0 65000 i
*> 172.16.1.1/32    10.100.2.1                             0 65000 65101 i
*> 172.16.2.1/32    0.0.0.0                  0         32768 i
*> 172.16.3.1/32    10.100.2.1                             0 65000 65103 i

Displayed  4 routes and 4 total paths
\end{verbatim}

\paragraph{RIB installation}

\begin{verbatim}
sudo vtysh -c "show ip route bgp"
\end{verbatim}

\begin{verbatim}
B>* 172.16.0.1/32 [20/0] via 10.100.2.1, wg0, weight 1, 00:07:17
B>* 172.16.1.1/32 [20/0] via 10.100.2.1, wg0, weight 1, 00:07:17
B>* 172.16.3.1/32 [20/0] via 10.100.2.1, wg0, weight 1, 00:07:15
\end{verbatim}

% -----------------------------------------------------------------------------
% 6.2.5 SPOKE BGP VIEW -- SITE3-LF1
%
% Final spoke validation confirming consistent overlay semantics
% across all sites.
%
% Expected State:
%   - Single neighbor in AS 65000.
%   - Three remote loopbacks learned via hub.
%   - Local loopback originated locally.
%
% Confirms full overlay mesh reachability via centralized transit model.
% -----------------------------------------------------------------------------
\subsubsection{6.2.5 Spoke BGP View -- SITE3-LF1}
\label{sec:bgp-overlay-spoke-site3}

\paragraph{SITE3-LF1 -- BGP Overlay View}

\paragraph{BGP neighbor status}

\begin{verbatim}
sudo vtysh -c "show ip bgp summary"
\end{verbatim}

\begin{verbatim}
IPv4 Unicast Summary (VRF default):
BGP router identifier 1.1.1.13, local AS number 65103 vrf-id 0
BGP table version 4
RIB entries 7, using 1288 bytes of memory
Peers 1, using 723 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd
        PfxSnt Desc
10.100.3.1      4      65000        14        14        0    0    0 00:07:45            3
        4 DC1-SP1

Total number of neighbors 1
\end{verbatim}

\paragraph{BGP table}

\begin{verbatim}
sudo vtysh -c "show ip bgp"
\end{verbatim}

\begin{verbatim}
   Network          Next Hop            Metric LocPrf Weight Path
*> 172.16.0.1/32    10.100.3.1               0             0 65000 i
*> 172.16.1.1/32    10.100.3.1                             0 65000 65101 i
*> 172.16.2.1/32    10.100.3.1                             0 65000 65102 i
*> 172.16.3.1/32    0.0.0.0                  0         32768 i

Displayed  4 routes and 4 total paths
\end{verbatim}

\paragraph{RIB installation}

\begin{verbatim}
sudo vtysh -c "show ip route bgp"
\end{verbatim}

\begin{verbatim}
B>* 172.16.0.1/32 [20/0] via 10.100.3.1, wg0, weight 1, 00:07:56
B>* 172.16.1.1/32 [20/0] via 10.100.3.1, wg0, weight 1, 00:07:56
B>* 172.16.2.1/32 [20/0] via 10.100.3.1, wg0, weight 1, 00:07:56
\end{verbatim}

% -----------------------------------------------------------------------------
% 6.2.6 BGP OVERLAY VALIDATION SUMMARY
%
% Combined hub and spoke outputs confirm:
%
%   - Full loopback reachability across all nodes.
%   - Exactly one eBGP adjacency per spoke.
%   - Hub AS 65000 functions as sole transit domain.
%   - Identical control-plane state across hub instances.
%
% The overlay operates strictly as a hub-and-spoke transit topology.
% No lateral spoke adjacency exists.
% -----------------------------------------------------------------------------
\subsubsection{6.2.6 BGP Overlay Validation Summary}
\label{sec:bgp-overlay-validation-summary}

From the combined hub and spoke views:

\begin{itemize}
  \item \textbf{Full mesh of loopback reachability:}
        All nodes have reachability to all four /32 loopbacks via the BGP overlay.
  \item \textbf{Single eBGP adjacency per spoke:}
        Each site forms exactly one eBGP session to the hub group at 10.100.X.1.
  \item \textbf{Hub as BGP transit:}
        Each spoke learns other sites’ loopbacks with AS paths of the form:
        \[
          65000 \; 6510X
        \]
        indicating that all inter-site routing transits the hub.
  \item \textbf{Identical hub control-plane state:}
        DC1-SP1 and DC1-SP2 maintain identical BGP RIB contents for the overlay.
  \item \textbf{Overlay routes sourced via BGP:}
        On spokes, all 172.16.X.1/32 routes are installed as \texttt{proto bgp} (Section~\ref{sec:wg-underlay-validation}); on hubs, BGP learning is validated via FRR RIB inspection.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.2.7 STATIC OVERLAY ROUTE VERIFICATION
%
% Explicit validation that no static routing constructs influence
% overlay reachability.
%
% Verification Scope:
%   - Kernel static route inspection.
%   - FRR staticd absence.
%   - Running configuration review.
%
% Result:
%   All 172.16.X.1/32 prefixes are sourced exclusively from BGP.
%
% This confirms dynamic control-plane dependency and eliminates
% configuration drift risk from static entries.
% -----------------------------------------------------------------------------
\subsubsection{6.2.7 Static Overlay Route Verification}
\label{sec:bgp-overlay-no-static}

To explicitly verify that overlay reachability is not dependent on static routing, the following checks were performed on all hubs and spokes:

\begin{itemize}
  \item \texttt{ip route show proto static} produced no output.
  \item \texttt{ip -4 route show | grep -i static} produced no output.
  \item The file \texttt{/etc/frr/staticd.conf} does not exist.
  \item \texttt{show running-config} contains no static routing statements.
\end{itemize}

These results confirm that no kernel-level or FRRouting static routes are configured for overlay prefixes. All 172.16.X.1/32 reachability observed in this section is therefore sourced exclusively from BGP.

This confirms that the BGP overlay behaves as intended and aligns with the underlay validation results presented in Section~\ref{sec:wg-underlay-validation}.

% -----------------------------------------------------------------------------
% 6.3 CONNECTIVITY VALIDATION AND FAILOVER VERIFICATION
%
% This subsection validates end-to-end Layer 3 forwarding behavior
% across steady-state, failover, and failback conditions.
%
% Validation confirms:
%   - Full inter-site reachability under normal operation.
%   - Correct hub-and-spoke forwarding paths.
%   - Bounded packet loss during active failover.
%   - Restoration of steady-state behavior after EIP reassociation.
%
% Testing methodology:
%   - ICMP echo for reachability and loss measurement.
%   - Traceroute for hop-by-hop path verification.
%
% Results are derived from live system output and reproduced verbatim.
% -----------------------------------------------------------------------------
\subsection{6.3 Connectivity Validation and Failover Verification}
\label{sec:connectivity-validation}

This section documents end-to-end Layer 3 connectivity validation between all participating sites and the data center prior to failover, during failover, and after failback. Validation is performed using ICMP echo requests and traceroute path inspection originating from each site Linux forwarding instance (LF1). All outputs below are captured directly from the test systems and are reproduced verbatim.

% -----------------------------------------------------------------------------
% 6.3.1 PRE-FAILOVER CONNECTIVITY VALIDATION
%
% Establishes steady-state baseline prior to initiating hub failover.
%
% Verification Scope:
%   - Hub loopback reachability from all spokes.
%   - Full spoke-to-spoke reachability via hub transit.
%   - Zero packet loss under normal operating conditions.
%
% Baseline measurements are used for comparison against failover
% and post-failback behavior.
% -----------------------------------------------------------------------------
\subsubsection{6.3.1 Pre-Failover Connectivity Validation}
\label{sec:pre-failover-validation}

Prior to initiating failover, ICMP and traceroute testing was performed between all sites and the primary data center service endpoint to establish a baseline of steady-state connectivity and routing behavior.

\paragraph{Pre-Failover ICMP Validation}

\begin{verbatim}
SITE1-LF1 to DC1-SP1:

ping -c 3 172.16.0.1
PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=17.7 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=17.9 ms

--- 172.16.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 17.717/17.899/18.090/0.152 ms
---
SITE1-LF1 to SITE2-LF1:

ping -c 3 172.16.2.1
PING 172.16.2.1 (172.16.2.1) 56(84) bytes of data.
64 bytes from 172.16.2.1: icmp_seq=1 ttl=63 time=34.5 ms
64 bytes from 172.16.2.1: icmp_seq=2 ttl=63 time=35.5 ms
64 bytes from 172.16.2.1: icmp_seq=3 ttl=63 time=36.0 ms

--- 172.16.2.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 34.450/35.315/35.992/0.643 ms
---
SITE1-LF1 to SITE3-LF1:

ping -c 3 172.16.3.1
PING 172.16.3.1 (172.16.3.1) 56(84) bytes of data.
64 bytes from 172.16.3.1: icmp_seq=1 ttl=63 time=35.6 ms
64 bytes from 172.16.3.1: icmp_seq=2 ttl=63 time=36.2 ms
64 bytes from 172.16.3.1: icmp_seq=3 ttl=63 time=36.3 ms

--- 172.16.3.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 35.582/36.045/36.339/0.331 ms
---
SITE2-LF1 to DC1-SP1:

ping -c 3 172.16.0.1
PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=17.3 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=17.6 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=17.3 ms

--- 172.16.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 17.263/17.391/17.568/0.129 ms
---
SITE2-LF1 to SITE1-LF1:

ping -c 3 172.16.1.1
PING 172.16.1.1 (172.16.1.1) 56(84) bytes of data.
64 bytes from 172.16.1.1: icmp_seq=1 ttl=63 time=35.2 ms
64 bytes from 172.16.1.1: icmp_seq=2 ttl=63 time=35.4 ms
64 bytes from 172.16.1.1: icmp_seq=3 ttl=63 time=35.6 ms

--- 172.16.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 35.180/35.402/35.602/0.172 ms
---
SITE2-LF1 to SITE3-LF1:

ping -c 3 172.16.3.1
PING 172.16.3.1 (172.16.3.1) 56(84) bytes of data.
64 bytes from 172.16.3.1: icmp_seq=1 ttl=63 time=34.6 ms
64 bytes from 172.16.3.1: icmp_seq=2 ttl=63 time=35.5 ms
64 bytes from 172.16.3.1: icmp_seq=3 ttl=63 time=35.9 ms

--- 172.16.3.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 34.567/35.316/35.910/0.559 ms
---
SITE3-LF1 to DC1-SP1:

ping -c 3 172.16.0.1
PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=17.8 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=17.9 ms

--- 172.16.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 17.436/17.712/17.919/0.203 ms
---
SITE3-LF1 to SITE1-LF1:

ping -c 3 172.16.1.1
PING 172.16.1.1 (172.16.1.1) 56(84) bytes of data.
64 bytes from 172.16.1.1: icmp_seq=1 ttl=63 time=35.5 ms
64 bytes from 172.16.1.1: icmp_seq=2 ttl=63 time=35.8 ms
64 bytes from 172.16.1.1: icmp_seq=3 ttl=63 time=35.5 ms

--- 172.16.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 35.491/35.609/35.833/0.158 ms
---
SITE3-LF1 to SITE2-LF1:

ping -c 3 172.16.2.1
PING 172.16.2.1 (172.16.2.1) 56(84) bytes of data.
64 bytes from 172.16.2.1: icmp_seq=1 ttl=63 time=34.6 ms
64 bytes from 172.16.2.1: icmp_seq=2 ttl=63 time=35.5 ms
64 bytes from 172.16.2.1: icmp_seq=3 ttl=63 time=35.2 ms

--- 172.16.2.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 34.636/35.121/35.528/0.368 ms
\end{verbatim}

Observations:

\begin{itemize}
  \item \textbf{Hub--spoke reachability:}
        Each spoke site (SITE1-LF1, SITE2-LF1, SITE3-LF1) successfully reaches the hub loopback
        address (172.16.0.1) with 0\% packet loss in steady state prior to failover.
  \item Round-trip times are consistent across spokes and align with expected underlay latency,
        indicating stable hub availability and correct EIP association to DC1-SP1.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.3.2 PRE-FAILOVER TRACEROUTE VALIDATION
%
% Validates hop-by-hop forwarding paths under steady-state operation.
%
% Expected Topology Behavior:
%   - Spoke-to-hub traffic: single hop.
%   - Spoke-to-spoke traffic: two hops via hub underlay IP.
%
% Confirms hub-and-spoke transit model and absence of alternate
% or asymmetric forwarding paths.
% -----------------------------------------------------------------------------
\subsubsection{6.3.2 Pre-Failover Traceroute Validation}
\label{sec:pre-failover-traceroute}

Traceroute testing was conducted prior to failover to validate Layer 3 forwarding paths between each site and the data center service endpoint, as well as inter-site routing behavior. The results below confirm expected hop-by-hop paths under normal operating conditions.

\begin{verbatim}
SITE1-LF1 to DC1-SP1:

traceroute 172.16.0.1
traceroute to 172.16.0.1 (172.16.0.1), 30 hops max, 60 byte packets
 1  172.16.0.1 (172.16.0.1)  17.202 ms  16.990 ms  16.891 ms
---
SITE1-LF1 to SITE2-LF1:

traceroute 172.16.2.1
traceroute to 172.16.2.1 (172.16.2.1), 30 hops max, 60 byte packets
 1  10.100.1.1 (10.100.1.1)  17.055 ms  16.603 ms  16.335 ms
 2  172.16.2.1 (172.16.2.1)  33.448 ms  33.194 ms  32.944 ms
---
SITE1-LF1 to SITE3-LF1:

traceroute 172.16.3.1
traceroute to 172.16.3.1 (172.16.3.1), 30 hops max, 60 byte packets
 1  10.100.1.1 (10.100.1.1)  16.640 ms  16.414 ms  16.266 ms
 2  172.16.3.1 (172.16.3.1)  33.280 ms  33.139 ms  32.982 ms
---
SITE2-LF1 to DC1-SP1:

traceroute 172.16.0.1
traceroute to 172.16.0.1 (172.16.0.1), 30 hops max, 60 byte packets
 1  172.16.0.1 (172.16.0.1)  16.942 ms  16.761 ms  16.654 ms
---
SITE2-LF1 to SITE1-LF1:

traceroute 172.16.1.1
traceroute to 172.16.1.1 (172.16.1.1), 30 hops max, 60 byte packets
 1  10.100.2.1 (10.100.2.1)  17.241 ms  17.049 ms  16.933 ms
 2  172.16.1.1 (172.16.1.1)  34.276 ms  34.164 ms  34.052 ms
---
SITE2-LF1 to SITE3-LF1:

traceroute 172.16.3.1
traceroute to 172.16.3.1 (172.16.3.1), 30 hops max, 60 byte packets
 1  10.100.2.1 (10.100.2.1)  17.138 ms  16.963 ms  16.869 ms
 2  172.16.3.1 (172.16.3.1)  34.036 ms  33.945 ms  33.847 ms
---
SITE3-LF1 to DC1-SP1:

traceroute 172.16.0.1
traceroute to 172.16.0.1 (172.16.0.1), 30 hops max, 60 byte packets
 1  172.16.0.1 (172.16.0.1)  16.809 ms  16.601 ms  16.461 ms
---
SITE3-LF1 to SITE1-LF1:

traceroute 172.16.1.1
traceroute to 172.16.1.1 (172.16.1.1), 30 hops max, 60 byte packets
 1  10.100.3.1 (10.100.3.1)  17.341 ms  17.323 ms  17.311 ms
 2  172.16.1.1 (172.16.1.1)  34.016 ms  33.982 ms  33.957 ms
---
SITE3-LF1 to SITE2-LF1:

traceroute 172.16.2.1
traceroute to 172.16.2.1 (172.16.2.1), 30 hops max, 60 byte packets
 1  10.100.3.1 (10.100.3.1)  16.513 ms  16.354 ms  16.265 ms
 2  172.16.2.1 (172.16.2.1)  33.103 ms  33.019 ms  32.934 ms
\end{verbatim}

Observations:

\begin{itemize}
  \item \textbf{Spoke--spoke reachability:}
        Each spoke can reach every other spoke loopback
        (172.16.1.1, 172.16.2.1, 172.16.3.1) with 0\% packet loss in steady state.
  \item \textbf{Forwarding paths:}
        Traceroute confirms the intended hub-and-spoke topology:
        \begin{itemize}
          \item One hop for spoke $\rightarrow$ hub loopback traffic.
          \item Two hops for spoke $\rightarrow$ spoke traffic, transiting the hub’s underlay IP.
        \end{itemize}
  \item No evidence of asymmetric routing or unintended path selection is observed.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.3.3 ACTIVE FAILOVER ICMP VALIDATION
%
% Observes connectivity behavior during active hub transition.
%
% Verification Focus:
%   - Packet loss window during Elastic IP reassociation.
%   - Forwarding interruption duration.
%   - Post-transition latency normalization.
%
% Failover event sequence:
%   1. Active hub instance termination.
%   2. EventBridge detection of state change.
%   3. Lambda-triggered Elastic IP reassignment.
%
% Successful validation requires bounded loss and automatic
% restoration of steady-state connectivity.
% -----------------------------------------------------------------------------
\subsubsection{6.3.3 Active Failover ICMP Validation}
\label{sec:active-failover-validation}

During the failover event, continuous ICMP echo requests were issued from SITE1-LF1 toward the data center service endpoint to observe packet loss and latency behavior while the service endpoint transitioned. The following output captures the full failover interval.

\begin{verbatim}
Full failover ping (from SITE1-LF1 to EIP):

PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=20.0 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=17.8 ms
64 bytes from 172.16.0.1: icmp_seq=4 ttl=64 time=17.0 ms
64 bytes from 172.16.0.1: icmp_seq=5 ttl=64 time=16.9 ms
64 bytes from 172.16.0.1: icmp_seq=6 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=7 ttl=64 time=16.8 ms
64 bytes from 172.16.0.1: icmp_seq=8 ttl=64 time=17.0 ms
64 bytes from 172.16.0.1: icmp_seq=9 ttl=64 time=17.0 ms
64 bytes from 172.16.0.1: icmp_seq=10 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=11 ttl=64 time=16.9 ms
64 bytes from 172.16.0.1: icmp_seq=12 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=28 ttl=64 time=17.3 ms
64 bytes from 172.16.0.1: icmp_seq=29 ttl=64 time=17.1 ms
64 bytes from 172.16.0.1: icmp_seq=30 ttl=64 time=17.1 ms
64 bytes from 172.16.0.1: icmp_seq=31 ttl=64 time=17.6 ms
64 bytes from 172.16.0.1: icmp_seq=32 ttl=64 time=17.8 ms
64 bytes from 172.16.0.1: icmp_seq=33 ttl=64 time=17.9 ms
64 bytes from 172.16.0.1: icmp_seq=34 ttl=64 time=18.0 ms
64 bytes from 172.16.0.1: icmp_seq=35 ttl=64 time=17.0 ms
64 bytes from 172.16.0.1: icmp_seq=36 ttl=64 time=17.5 ms
64 bytes from 172.16.0.1: icmp_seq=37 ttl=64 time=17.9 ms
64 bytes from 172.16.0.1: icmp_seq=38 ttl=64 time=17.7 ms
64 bytes from 172.16.0.1: icmp_seq=39 ttl=64 time=17.6 ms
64 bytes from 172.16.0.1: icmp_seq=40 ttl=64 time=17.8 ms
64 bytes from 172.16.0.1: icmp_seq=41 ttl=64 time=17.8 ms
64 bytes from 172.16.0.1: icmp_seq=42 ttl=64 time=17.9 ms
64 bytes from 172.16.0.1: icmp_seq=43 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=44 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=45 ttl=64 time=17.0 ms
64 bytes from 172.16.0.1: icmp_seq=46 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=47 ttl=64 time=17.0 ms
64 bytes from 172.16.0.1: icmp_seq=48 ttl=64 time=17.0 ms

--- 172.16.0.1 ping statistics ---
48 packets transmitted, 33 received, 31.25% packet loss, time 47414ms
rtt min/avg/max/mdev = 16.836/17.428/20.027/0.578 ms
\end{verbatim}

Observations:

\begin{itemize}
  \item \textbf{Failover behavior:}
        When DC1-SP1 is stopped, the Elastic IP is automatically disassociated and
        reassociated to DC1-SP2 without manual intervention.
  \item A bounded packet loss window is observed during the transition, corresponding to:
        \begin{enumerate}
          \item DC1-SP1 instance shutdown,
          \item EventBridge detection of the instance state change,
          \item Execution of the failover Lambda function and EIP reassociation.
        \end{enumerate}
  \item Following reassociation, hub--spoke and spoke--spoke connectivity resumes with
        steady-state characteristics consistent with pre-failover behavior.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.3.4 FAILBACK ICMP VALIDATION
%
% Observes connectivity behavior during restoration of the primary hub.
%
% Verification Focus:
%   - Packet loss window during reverse Elastic IP reassociation.
%   - Boot-to-service restoration timing.
%   - Post-transition latency normalization.
%
% Failback event sequence:
%   1. Primary hub instance boot completion.
%   2. EventBridge detection of running state.
%   3. Lambda-triggered Elastic IP reassignment to primary hub.
%
% Successful validation requires bounded loss and restoration of
% baseline steady-state forwarding characteristics.
% -----------------------------------------------------------------------------
\subsubsection{6.3.4 Failback ICMP Validation}
\label{sec:failback-validation}

Following restoration of the primary service endpoint, continuous ICMP echo requests were again issued from SITE1-LF1 toward the data center service endpoint to observe packet delivery and latency characteristics during the failback process. The complete captured output is shown below.

\begin{verbatim}
Full failback ping (from SITE1-LF1 to EIP):

PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=17.7 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=17.5 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=17.9 ms
64 bytes from 172.16.0.1: icmp_seq=4 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=5 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=6 ttl=64 time=17.1 ms
64 bytes from 172.16.0.1: icmp_seq=7 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=8 ttl=64 time=17.3 ms
64 bytes from 172.16.0.1: icmp_seq=9 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=10 ttl=64 time=17.3 ms
64 bytes from 172.16.0.1: icmp_seq=11 ttl=64 time=17.1 ms
64 bytes from 172.16.0.1: icmp_seq=12 ttl=64 time=17.1 ms
64 bytes from 172.16.0.1: icmp_seq=13 ttl=64 time=16.6 ms
64 bytes from 172.16.0.1: icmp_seq=49 ttl=64 time=18.0 ms
64 bytes from 172.16.0.1: icmp_seq=50 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=51 ttl=64 time=17.9 ms
64 bytes from 172.16.0.1: icmp_seq=52 ttl=64 time=18.0 ms
64 bytes from 172.16.0.1: icmp_seq=53 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=54 ttl=64 time=18.3 ms
64 bytes from 172.16.0.1: icmp_seq=55 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=56 ttl=64 time=18.5 ms
64 bytes from 172.16.0.1: icmp_seq=57 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=58 ttl=64 time=18.3 ms
64 bytes from 172.16.0.1: icmp_seq=59 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=60 ttl=64 time=17.9 ms
64 bytes from 172.16.0.1: icmp_seq=61 ttl=64 time=18.3 ms
64 bytes from 172.16.0.1: icmp_seq=62 ttl=64 time=17.6 ms
64 bytes from 172.16.0.1: icmp_seq=63 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=64 ttl=64 time=18.7 ms
64 bytes from 172.16.0.1: icmp_seq=65 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=66 ttl=64 time=17.5 ms
64 bytes from 172.16.0.1: icmp_seq=67 ttl=64 time=17.6 ms
64 bytes from 172.16.0.1: icmp_seq=68 ttl=64 time=17.7 ms
64 bytes from 172.16.0.1: icmp_seq=69 ttl=64 time=18.0 ms
64 bytes from 172.16.0.1: icmp_seq=70 ttl=64 time=18.0 ms
64 bytes from 172.16.0.1: icmp_seq=71 ttl=64 time=18.3 ms
64 bytes from 172.16.0.1: icmp_seq=72 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=73 ttl=64 time=17.7 ms
64 bytes from 172.16.0.1: icmp_seq=74 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=75 ttl=64 time=18.1 ms
64 bytes from 172.16.0.1: icmp_seq=76 ttl=64 time=18.2 ms
64 bytes from 172.16.0.1: icmp_seq=77 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=78 ttl=64 time=17.8 ms
64 bytes from 172.16.0.1: icmp_seq=79 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=80 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=81 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=82 ttl=64 time=17.3 ms
64 bytes from 172.16.0.1: icmp_seq=83 ttl=64 time=17.2 ms
64 bytes from 172.16.0.1: icmp_seq=84 ttl=64 time=17.4 ms
64 bytes from 172.16.0.1: icmp_seq=85 ttl=64 time=17.3 ms

--- 172.16.0.1 ping statistics ---
85 packets transmitted, 50 received, 41.1765% packet loss, time 84915ms
rtt min/avg/max/mdev = 16.626/17.705/18.748/0.460 ms
\end{verbatim}

Observations:

\begin{itemize}
  \item \textbf{Failback behavior:}
        When DC1-SP1 is restarted, the Elastic IP is automatically moved back from
        DC1-SP2 to DC1-SP1.
  \item As in the failover case, a transient outage window is observed while:
        \begin{enumerate}
          \item DC1-SP1 completes boot,
          \item EventBridge detects the \texttt{running} state,
          \item The failback Lambda disassociates the EIP from DC1-SP2 and reassociates it to DC1-SP1.
        \end{enumerate}
  \item Once failback completes, steady-state RTTs and forwarding behavior match the
        original DC1-SP1-active baseline.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.3.5 POST-FAILOVER STEADY-STATE VALIDATION
%
% Revalidates full connectivity after standby hub assumes active role.
%
% Verification Scope:
%   - Hub loopback reachability from all spokes.
%   - Full spoke-to-spoke reachability via hub transit.
%   - Zero packet loss under steady-state conditions.
%   - Forwarding path consistency with pre-failover topology.
%
% Confirms that failover does not alter overlay routing behavior
% or forwarding semantics.
% -----------------------------------------------------------------------------
\subsubsection{6.3.5 Post-Failover Steady-State Connectivity Validation}
\label{sec:post-failover-validation}

After failover completed and DC1-SP2 assumed the active role holding the Elastic IP, steady-state connectivity was revalidated across all spokes. ICMP echo and traceroute tests were executed to confirm restoration of hub--spoke and spoke--spoke reachability and to verify that forwarding paths remained unchanged.

\paragraph{Post-Failover ICMP Validation}

\subparagraph{SITE1-LF1 $\rightarrow$ DC1-SP2 (172.16.0.1)}

\begin{verbatim}
ping -c 3 172.16.0.1
PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=23.3 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=22.6 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=22.4 ms

--- 172.16.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 22.353/22.763/23.298/0.395 ms
\end{verbatim}

\subparagraph{SITE1-LF1 $\rightarrow$ SITE2-LF1 (172.16.2.1)}

\begin{verbatim}
ping -c 3 172.16.2.1
PING 172.16.2.1 (172.16.2.1) 56(84) bytes of data.
64 bytes from 172.16.2.1: icmp_seq=1 ttl=63 time=44.5 ms
64 bytes from 172.16.2.1: icmp_seq=2 ttl=63 time=44.4 ms
64 bytes from 172.16.2.1: icmp_seq=3 ttl=63 time=44.6 ms

--- 172.16.2.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 44.351/44.498/44.597/0.106 ms
\end{verbatim}

\subparagraph{SITE1-LF1 $\rightarrow$ SITE3-LF1 (172.16.3.1)}

\begin{verbatim}
ping -c 3 172.16.3.1
PING 172.16.3.1 (172.16.3.1) 56(84) bytes of data.
64 bytes from 172.16.3.1: icmp_seq=1 ttl=63 time=45.0 ms
64 bytes from 172.16.3.1: icmp_seq=2 ttl=63 time=44.6 ms
64 bytes from 172.16.3.1: icmp_seq=3 ttl=63 time=45.1 ms

--- 172.16.3.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 44.586/44.910/45.099/0.230 ms
\end{verbatim}

\subparagraph{SITE2-LF1 $\rightarrow$ DC1-SP2 (172.16.0.1)}

\begin{verbatim}
ping -c 3 172.16.0.1
PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=22.5 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=22.2 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=22.8 ms

--- 172.16.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 22.191/22.500/22.771/0.238 ms
\end{verbatim}

\subparagraph{SITE2-LF1 $\rightarrow$ SITE1-LF1 (172.16.1.1)}

\begin{verbatim}
ping -c 3 172.16.1.1
PING 172.16.1.1 (172.16.1.1) 56(84) bytes of data.
64 bytes from 172.16.1.1: icmp_seq=1 ttl=63 time=44.5 ms
64 bytes from 172.16.1.1: icmp_seq=2 ttl=63 time=44.9 ms
64 bytes from 172.16.1.1: icmp_seq=3 ttl=63 time=44.6 ms

--- 172.16.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 44.506/44.646/44.879/0.165 ms
\end{verbatim}

\subparagraph{SITE2-LF1 $\rightarrow$ SITE3-LF1 (172.16.3.1)}

\begin{verbatim}
ping -c 3 172.16.3.1
PING 172.16.3.1 (172.16.3.1) 56(84) bytes of data.
64 bytes from 172.16.3.1: icmp_seq=1 ttl=63 time=45.4 ms
64 bytes from 172.16.3.1: icmp_seq=2 ttl=63 time=45.3 ms
64 bytes from 172.16.3.1: icmp_seq=3 ttl=63 time=44.8 ms

--- 172.16.3.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 44.778/45.143/45.354/0.259 ms
\end{verbatim}

\subparagraph{SITE3-LF1 $\rightarrow$ DC1-SP2 (172.16.0.1)}

\begin{verbatim}
ping -c 3 172.16.0.1
PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data.
64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=23.8 ms
64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=22.4 ms
64 bytes from 172.16.0.1: icmp_seq=3 ttl=64 time=22.7 ms

--- 172.16.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 22.395/22.979/23.810/0.603 ms
\end{verbatim}

\subparagraph{SITE3-LF1 $\rightarrow$ SITE1-LF1 (172.16.1.1)}

\begin{verbatim}
ping -c 3 172.16.1.1
PING 172.16.1.1 (172.16.1.1) 56(84) bytes of data.
64 bytes from 172.16.1.1: icmp_seq=1 ttl=63 time=45.2 ms
64 bytes from 172.16.1.1: icmp_seq=2 ttl=63 time=45.0 ms
64 bytes from 172.16.1.1: icmp_seq=3 ttl=63 time=44.9 ms

--- 172.16.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 44.927/45.044/45.169/0.098 ms
\end{verbatim}

\subparagraph{SITE3-LF1 $\rightarrow$ SITE2-LF1 (172.16.2.1)}

\begin{verbatim}
ping -c 3 172.16.2.1
PING 172.16.2.1 (172.16.2.1) 56(84) bytes of data.
64 bytes from 172.16.2.1: icmp_seq=1 ttl=63 time=45.3 ms
64 bytes from 172.16.2.1: icmp_seq=2 ttl=63 time=44.9 ms
64 bytes from 172.16.2.1: icmp_seq=3 ttl=63 time=44.9 ms

--- 172.16.2.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 44.909/45.048/45.292/0.172 ms
\end{verbatim}

\paragraph{Post-Failover Traceroute Validation}

\subparagraph{SITE1-LF1 $\rightarrow$ DC1-SP2}

\begin{verbatim}
traceroute 172.16.0.1
traceroute to 172.16.0.1 (172.16.0.1), 30 hops max, 60 byte packets
 1  172.16.0.1 (172.16.0.1)  17.154 ms  17.064 ms  16.992 ms
\end{verbatim}

\subparagraph{SITE1-LF1 $\rightarrow$ SITE2-LF1}

\begin{verbatim}
traceroute 172.16.2.1
traceroute to 172.16.2.1 (172.16.2.1), 30 hops max, 60 byte packets
 1  10.100.1.1 (10.100.1.1)  17.204 ms  16.791 ms  16.413 ms
 2  172.16.2.1 (172.16.2.1)  33.540 ms  33.315 ms  32.872 ms
\end{verbatim}

\subparagraph{SITE1-LF1 $\rightarrow$ SITE3-LF1}

\begin{verbatim}
traceroute 172.16.3.1
traceroute to 172.16.3.1 (172.16.3.1), 30 hops max, 60 byte packets
 1  10.100.1.1 (10.100.1.1)  17.120 ms  16.691 ms  16.344 ms
 2  172.16.3.1 (172.16.3.1)  33.108 ms  32.719 ms  32.507 ms
\end{verbatim}

\subparagraph{SITE2-LF1 $\rightarrow$ DC1-SP2}

\begin{verbatim}
traceroute 172.16.0.1
traceroute to 172.16.0.1 (172.16.0.1), 30 hops max, 60 byte packets
 1  172.16.0.1 (172.16.0.1)  17.013 ms  16.794 ms  16.584 ms
\end{verbatim}

\subparagraph{SITE2-LF1 $\rightarrow$ SITE1-LF1}

\begin{verbatim}
traceroute 172.16.1.1
traceroute to 172.16.1.1 (172.16.1.1), 30 hops max, 60 byte packets
 1  10.100.2.1 (10.100.2.1)  17.181 ms  16.843 ms  16.621 ms
 2  172.16.1.1 (172.16.1.1)  34.096 ms  33.947 ms  33.391 ms
\end{verbatim}

\subparagraph{SITE2-LF1 $\rightarrow$ SITE3-LF1}

\begin{verbatim}
traceroute 172.16.3.1
traceroute to 172.16.3.1 (172.16.3.1), 30 hops max, 60 byte packets
 1  10.100.2.1 (10.100.2.1)  16.756 ms  16.361 ms  16.210 ms
 2  172.16.3.1 (172.16.3.1)  34.121 ms  33.899 ms  33.702 ms
\end{verbatim}

\subparagraph{SITE3-LF1 $\rightarrow$ DC1-SP2}

\begin{verbatim}
traceroute 172.16.0.1
traceroute to 172.16.0.1 (172.16.0.1), 30 hops max, 60 byte packets
 1  172.16.0.1 (172.16.0.1)  16.954 ms  16.771 ms  16.480 ms
\end{verbatim}

\subparagraph{SITE3-LF1 $\rightarrow$ SITE1-LF1}

\begin{verbatim}
traceroute 172.16.1.1
traceroute to 172.16.1.1 (172.16.1.1), 30 hops max, 60 byte packets
 1  10.100.3.1 (10.100.3.1)  17.029 ms  16.834 ms  16.791 ms
 2  172.16.1.1 (172.16.1.1)  34.024 ms  33.807 ms  33.736 ms
\end{verbatim}

\subparagraph{SITE3-LF1 $\rightarrow$ SITE2-LF1}

\begin{verbatim}
traceroute 172.16.2.1
traceroute to 172.16.2.1 (172.16.2.1), 30 hops max, 60 byte packets
 1  10.100.3.1 (10.100.3.1)  16.429 ms  16.336 ms  16.098 ms
 2  172.16.2.1 (172.16.2.1)  33.071 ms  32.857 ms  32.694 ms
\end{verbatim}

Observations:

\begin{itemize}
  \item \textbf{Overlay stability:}
        At no point during failover or failback are the spoke configurations modified.
  \item All control-plane dynamics are contained within AWS infrastructure
        (instance state changes, health evaluation, and EIP reassociation).
  \item From the spokes’ perspective, the WireGuard underlay and BGP overlay remain stable,
        with no requirement for session resets or configuration changes.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.3.6 END-TO-END CONNECTIVITY VALIDATION SUMMARY
%
% Combined steady-state and transition testing confirms:
%
%   - Full hub–spoke and spoke–spoke reachability in all active states.
%   - Automatic Elastic IP reassociation without spoke reconfiguration.
%   - Bounded packet loss during hub transitions.
%   - Restoration of baseline latency and forwarding paths after events.
%
% Validation demonstrates that high-availability objectives are met
% while preserving overlay stability and operational simplicity.
% -----------------------------------------------------------------------------
\subsubsection{6.3.6 End-to-End Connectivity Validation Summary}

Across all pre-failover, failover, and failback test cases, the following end-to-end behavior is confirmed:

\begin{itemize}
  \item The hub-and-spoke topology provides full hub--spoke and spoke--spoke reachability
        with 0\% packet loss in steady state when either DC1-SP1 or DC1-SP2 is active.
  \item Elastic IP reassociation between hub instances occurs automatically based on
        instance state, with no manual intervention and no configuration changes on spokes.
  \item Failover and failback events introduce a short, bounded packet loss window that
        aligns with expected AWS instance state transitions and control-plane automation.
  \item Following each transition, steady-state latency and forwarding paths return to
        values consistent with the original baseline.
  \item The overall design satisfies high-availability objectives while preserving
        overlay stability and operational simplicity at the spoke sites.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.4 HIGH-AVAILABILITY AUTOMATION (AWS)
%
% This subsection documents the control-plane automation responsible for
% active/standby hub role management.
%
% Architectural Intent:
%   - Present a single stable public endpoint (Elastic IP).
%   - Reassign that endpoint automatically based on hub lifecycle
%     state and health.
%   - Preserve static spoke configuration across hub transitions.
%
% Automation scope is limited to AWS-native services; no routing or
% configuration changes occur on spoke systems during failover.
% -----------------------------------------------------------------------------
\subsection{6.4 High-Availability Automation (AWS)}
\label{sec:ha-automation-aws}

This section documents the automated active/standby hub failover and failback mechanism implemented in AWS. The intent is to keep the spoke configuration static by presenting a single stable public endpoint (Elastic IP) for the hub, while AWS automation re-associates that Elastic IP between two hub EC2 instances based on hub state and health.

\subsubsection{6.4.1 HA Overview and Operating Model}

\paragraph{HA objective}
Two EC2 hub instances are deployed:
\begin{itemize}
  \item \textbf{DC1-SP1} (preferred primary)
  \item \textbf{DC1-SP2} (secondary)
\end{itemize}
Only one hub is intended to be externally active at a time, defined as ``currently associated with the Elastic IP (EIP).''

\paragraph{Spoke behavior (no configuration changes during failover)}
All spokes terminate WireGuard sessions to the hub via the EIP rather than instance-specific public IPs. This provides:
\begin{itemize}
  \item A single stable endpoint for all spokes.
  \item Hub replacement without updating spoke configuration.
  \item Deterministic operational behavior during failover/failback.
\end{itemize}

\paragraph{Control-plane mechanisms}
Two mechanisms are used in tandem:
\begin{enumerate}
  \item \textbf{EventBridge rules (primary)}: drive deterministic EIP ownership based on DC1-SP1 instance lifecycle state.
  \item \textbf{CloudWatch alarm (contingency + telemetry)}: triggers failover if DC1-SP1 is running but unhealthy, and provides a native audit trail of health degradation.
\end{enumerate}

% -----------------------------------------------------------------------------
% Redaction Note:
%   Private IDs and public IP endpoints are intentionally redacted.
% -----------------------------------------------------------------------------
\subsubsection{6.4.2 AWS Resources and Identifiers (Lab-Specific)}

\begin{itemize}
  \item Region: \texttt{us-east-1}
  \item Hub instance type: \texttt{t3.micro}
  \item Elastic IP Allocation ID: \texttt{<REDACTED>}
  \item DC1-SP1 instance ID (primary): \texttt{<REDACTED>}
  \item DC1-SP2 instance ID (secondary): \texttt{<REDACTED>}
  \item Lambda (failover) function: \texttt{EIP-Failover-DC1}
  \item Lambda (failback) function: \texttt{EIP-Failback-DC1}
  \item Lambda execution role: \texttt{Lambda-EIP-Failover-Role}
  \item EventBridge rules:
    \begin{itemize}
      \item \texttt{Failover-EIP-on-DC1-SP1-Instance-Stoppage}
      \item \texttt{Failback-EIP-on-DC1-SP1-Instance-Start}
      \item \texttt{Failover-EIP-on-DC1-SP1-Alarm}
    \end{itemize}
  \item CloudWatch alarm: \texttt{DC1-SP1-StatusFailed-Alarm}
\end{itemize}

% -----------------------------------------------------------------------------
% 6.4.3 EVENTBRIDGE-DRIVEN FAILOVER AND FAILBACK
%
% Defines deterministic EIP ownership based on primary hub lifecycle state.
%
% Preference Model:
%   - DC1-SP1 running  -> DC1-SP1 holds EIP.
%   - DC1-SP1 stopped  -> DC1-SP2 holds EIP.
%
% EventBridge rules provide state-driven, infrastructure-native
% orchestration without dataplane awareness.
% -----------------------------------------------------------------------------
\subsubsection{6.4.3 EventBridge-Driven Failover and Failback}

\paragraph{Failover trigger: DC1-SP1 stopping/stopped}
The EventBridge rule \texttt{Failover-EIP-on-DC1-SP1-Instance-Stoppage} matches the EC2 instance state-change event for DC1-SP1 and triggers failover when the state becomes \texttt{stopping} or \texttt{stopped}.

\begin{verbatim}
Failover-EIP-on-DC1-SP1-Instance-Stoppage (Event Pattern)
{
  "source": ["aws.ec2"],
  "detail-type": ["EC2 Instance State-change Notification"],
  "detail": {
    "instance-id": ["<REDACTED>"],
    "state": ["stopping", "stopped"]
  }
}
\end{verbatim}

\paragraph{Failback trigger: DC1-SP1 running}
The EventBridge rule \texttt{Failback-EIP-on-DC1-SP1-Instance-Start} matches the EC2 instance state-change event for DC1-SP1 and triggers failback when the state becomes \texttt{running}.

\begin{verbatim}
Failback-EIP-on-DC1-SP1-Instance-Start (Event Pattern)
{
  "source": ["aws.ec2"],
  "detail-type": ["EC2 Instance State-change Notification"],
  "detail": {
    "instance-id": ["<REDACTED>"],
    "state": ["running"]
  }
}
\end{verbatim}

\paragraph{Operational outcome}
These two rules establish a deterministic preference model:
\begin{itemize}
  \item If \textbf{DC1-SP1 is running}, \textbf{DC1-SP1 should hold the EIP} (failback behavior).
  \item If \textbf{DC1-SP1 is stopping or stopped}, \textbf{DC1-SP2 should hold the EIP} (failover behavior).
\end{itemize}

% -----------------------------------------------------------------------------
% 6.4.4 CLOUDWATCH HEALTH-BASED FAILOVER CONTINGENCY
%
% Extends lifecycle-based automation to cover the
% "running but unhealthy" condition.
%
% Alarm-driven failover:
%   - Triggers when instance-level status checks fail.
%   - Invokes the same failover Lambda used for stoppage events.
%
% Failback remains exclusively lifecycle-driven to ensure the
% preferred primary resumes control only after confirmed running state.
% -----------------------------------------------------------------------------
\subsubsection{6.4.4 CloudWatch Alarm as Health-Based Contingency (and Telemetry)}

\paragraph{Problem addressed}
EC2 instance lifecycle state alone does not represent ``running but unhealthy.'' To cover this case, CloudWatch alarms are used to detect instance-level health check failure while the instance remains in the \texttt{running} state.

\paragraph{Alarm configuration (record of settings)}
\begin{itemize}
  \item Alarm name: \texttt{DC1-SP1-StatusFailed-Alarm}
  \item Namespace: \texttt{AWS/EC2}
  \item Metric name: \texttt{StatusCheckFailed\_Instance}
  \item Threshold: \texttt{StatusCheckFailed\_Instance > 0 for 2 datapoints within 2 minutes}
  \item Period: \texttt{1 minute}
  \item Datapoints to alarm: \texttt{2 out of 2}
  \item Missing data treatment: \texttt{Treat missing data as missing}
\end{itemize}

\paragraph{Alarm-to-Lambda trigger (EventBridge rule)}
The EventBridge rule \texttt{Failover-EIP-on-DC1-SP1-Alarm} matches CloudWatch alarm transitions into \texttt{ALARM} and invokes the same failover Lambda used for instance stoppage.

\begin{verbatim}
Failover-EIP-on-DC1-SP1-Alarm (Event Pattern)
{
  "source": ["aws.cloudwatch"],
  "detail-type": ["CloudWatch Alarm State Change"],
  "detail": {
    "alarmName": ["DC1-SP1-StatusFailed-Alarm"],
    "state": {
      "value": ["ALARM"]
    }
  }
}
\end{verbatim}

\paragraph{Scope limitation}
CloudWatch-based automation is used for \textbf{failover only}. Failback remains \textbf{exclusive to EC2 running-state detection} so the environment returns to DC1-SP1 only when it is confirmed \texttt{running} again.

% -----------------------------------------------------------------------------
% 6.4.5 LAMBDA CONTROL LOGIC
%
% Documents idempotent EIP reassociation logic.
%
% Design Properties:
%   - Safe no-op if desired state already satisfied.
%   - Explicit disassociate before reassociate.
%   - AllowReassociation enabled to ensure deterministic ownership.
%
% Lambda functions act purely at the infrastructure layer and do not
% modify routing, WireGuard configuration, or BGP state.
% -----------------------------------------------------------------------------
\subsubsection{6.4.5 Lambda Functions (Failover and Failback)}

The following blocks represent the logical control flow of each Lambda function. They are provided as pseudocode to document decision-making and AWS API interactions rather than as literal source code.

\paragraph{Failover Lambda: \texttt{EIP-Failover-DC1}}

\begin{verbatim}
EIP-Failover-DC1 (code)
- describe_addresses(AllocationIds=[<REDACTED>])
- if current == SECONDARY: no-op
- disassociate_address(AssociationId=association_id)
- associate_address(AllocationId=<REDACTED>,
                    InstanceId=SECONDARY,
                    AllowReassociation=True)
\end{verbatim}

\paragraph{Failback Lambda: \texttt{EIP-Failback-DC1}}

\begin{verbatim}
EIP-Failback-DC1 (code)
- describe_addresses(AllocationIds=[<REDACTED>])
- if current == PRIMARY: no-op
- disassociate_address(AssociationId=association_id)
- associate_address(AllocationId=<REDACTED>,
                    InstanceId=PRIMARY,
                    AllowReassociation=True)
\end{verbatim}

% -----------------------------------------------------------------------------
% 6.4.6 AUTOMATION EVIDENCE (CLOUDWATCH LOGS)
%
% CloudWatch Logs excerpts demonstrating:
%   - Failover EIP dissociation from DC1-SP1 and
%     reassociation with DC1-SP2.
%   - Failback EIP dissociation from DC1-SP2 and
%     reassociation with DC1-SP1.
%
% Redaction Note:
%   Private IDs and public IP endpoints are intentionally redacted.
% -----------------------------------------------------------------------------
\subsubsection{6.4.6 Automation Evidence (CloudWatch Logs)}

\paragraph{Failover evidence (\texttt{EIP-Failover-DC1})}
\begin{verbatim}
Current EIP holder: <REDACTED>, association: <REDACTED>
Disassociating from <REDACTED>...
Associating EIP <REDACTED> to SECONDARY <REDACTED>...
...
Current EIP holder: <REDACTED>, association: <REDACTED>
EIP already on SECONDARY -- no action needed.
\end{verbatim}

\paragraph{Failback evidence (\texttt{EIP-Failback-DC1})}
\begin{verbatim}
Received event: {... 'detail': {'instance-id': '<REDACTED>', 'state': 'running'}}
Current EIP holder: <REDACTED>, association: <REDACTED>
Disassociating from <REDACTED>...
Associating EIP <REDACTED> to PRIMARY <REDACTED>...
\end{verbatim}

% -----------------------------------------------------------------------------
% 6.4.7 VALIDATION LINKAGE
%
% Correlates AWS automation behavior with dataplane observations
% recorded in Section 6.3.
%
% Summary:
%   - EIP reassociation produces bounded packet loss.
%   - Connectivity resumes automatically.
%   - Overlay target (172.16.0.1) remains constant.
%
% Establishes causal linkage between control-plane automation and
% observed forwarding behavior.
% -----------------------------------------------------------------------------
\subsubsection{6.4.7 Validation Linkage}

End-to-end validation of these automations is shown operationally in Section~\ref{sec:connectivity-validation} (continuous ping during failover and failback). In addition to dataplane continuity, Section~\ref{sec:connectivity-validation} demonstrates that:
\begin{itemize}
  \item the EIP reassociation window produces bounded packet loss;
  \item reachability resumes automatically without spoke reconfiguration;
  \item the hub loopback address (\texttt{172.16.0.1}) remains the stable overlay target across hub transitions.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.5 SECURITY VALIDATION
%
% This subsection validates the implemented security posture of the lab.
%
% Validation scope:
%   - Public ingress restriction at AWS perimeter.
%   - Encryption and authentication of WAN traffic.
%   - Kernel and host configuration relevant to routed behavior.
%
% This is a proof-of-concept validation, not a production
% hardening guide.
% -----------------------------------------------------------------------------
\subsection{6.5 Security Validation}
\label{sec:security-validation}

This section validates the security posture of the lab environment as implemented, with explicit attention to ingress control, host-level packet handling, and cryptographic properties of the WireGuard underlay. The intent is not to claim production hardening, but to demonstrate that all security-relevant components required for a realistic small-enterprise WAN proof of concept are present, understood, and correctly configured.

% -----------------------------------------------------------------------------
% 6.5.1 THREAT MODEL AND VALIDATION SCOPE
%
% Defines boundaries of security analysis.
%
% Included:
%   - Ingress control.
%   - Encryption and authentication.
%   - Forwarding correctness without unintended filtering.
%
% Excluded:
%   - Operational hardening (IDS/IPS, DDoS strategy, automated rotation).
% -----------------------------------------------------------------------------
\subsubsection{6.5.1 Threat Model and Scope}

\paragraph{Scope of validation}
Security validation in this lab focuses on:
\begin{itemize}
  \item Controlled exposure of hub instances to the public Internet.
  \item Encryption and authentication of all WAN traffic.
  \item Absence of unintended filtering or stateful interference that could obscure dataplane or control-plane behavior.
\end{itemize}

The following areas are explicitly out of scope for this lab:
\begin{itemize}
  \item Key rotation automation.
  \item Intrusion detection / prevention systems.
  \item DDoS mitigation beyond default AWS protections.
  \item Host hardening beyond baseline Ubuntu defaults.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.5.2 AWS SECURITY GROUP CONTROLS
%
% Validates minimal exposure of hub instances.
%
% Design Properties:
%   - SSH restricted to single trusted source.
%   - WireGuard UDP restricted to expected peer source.
%   - No broad Internet exposure.
%
% Security Groups provide the primary perimeter control layer.
% -----------------------------------------------------------------------------
\subsubsection{6.5.2 AWS Security Group Controls (Hub Ingress)}

\paragraph{Security group intent}
The hub EC2 instances are protected by a minimal inbound security group that allows:
\begin{itemize}
  \item Administrative access (SSH) from a single trusted source.
  \item WireGuard UDP traffic only from a single trusted source.
\end{itemize}

No other inbound services are exposed.

\paragraph{Inbound rules (documented state)}

\begin{itemize}
  \item \textbf{SSH access}
    \begin{itemize}
      \item Protocol: TCP
      \item Port: 22
      \item Source: \texttt{<REDACTED: PERSONAL PUBLIC IP>}
      \item Description: SSH
    \end{itemize}

  \item \textbf{WireGuard access}
    \begin{itemize}
      \item Protocol: UDP
      \item Ports: 51820--51822
      \item Source: \texttt{<REDACTED: PERSONAL PUBLIC IP>}
      \item Description: WireGuard
    \end{itemize}
\end{itemize}

\paragraph{Security implications}
\begin{itemize}
  \item WireGuard listeners are reachable only from the expected spoke source IP.
  \item No broad \texttt{0.0.0.0/0} exposure exists for management or VPN traffic.
  \item The limited port range directly reflects the hub interface model (one UDP port per spoke-facing interface).
\end{itemize}

This aligns with the lab’s goal of realism without unnecessary complexity.

% -----------------------------------------------------------------------------
% 6.5.3 HOST FIREWALL CONFIGURATION
%
% Documents absence of host-level packet filtering.
%
% Rationale:
%   - Eliminates ambiguity during routing and convergence validation.
%   - Delegates ingress restriction to AWS perimeter controls.
%
% Production deployments would typically layer host firewalling
% on top of this baseline.
% -----------------------------------------------------------------------------
\subsubsection{6.5.3 Host Firewall Configuration (Linux)}

\paragraph{Observed configuration}
All hubs and spokes return identical output for \texttt{iptables -S}:

\begin{verbatim}
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
\end{verbatim}

\paragraph{Interpretation}
\begin{itemize}
  \item No host-level packet filtering is enforced.
  \item All traffic control is delegated to:
    \begin{itemize}
      \item AWS Security Groups (for public-facing ingress).
      \item Routing and WireGuard policy (for overlay behavior).
    \end{itemize}
\end{itemize}

\paragraph{Design rationale}
For a network-focused proof of concept:
\begin{itemize}
  \item Eliminating host firewall rules reduces ambiguity during troubleshooting.
  \item Routing, BGP convergence, and WireGuard behavior can be validated without interference from stateful filtering.
\end{itemize}

In a production deployment, host-based firewalling would typically be layered on top of this baseline.

% -----------------------------------------------------------------------------
% 6.5.4 IP FORWARDING CONFIGURATION
%
% Confirms kernel forwarding is enabled on hubs and spokes.
%
% Required for:
%   - Hub transit behavior.
%   - Spoke-to-spoke routed communication.
%
% Establishes routed WAN semantics rather than host-only tunneling.
% -----------------------------------------------------------------------------
\subsubsection{6.5.4 IP Forwarding and Packet Handling}

\paragraph{Kernel forwarding state}
All hubs and spokes report:

\begin{verbatim}
net.ipv4.ip_forward = 1
\end{verbatim}

\paragraph{Implications}
\begin{itemize}
  \item Hubs are able to forward traffic between WireGuard interfaces.
  \item Spokes are able to forward traffic between local LAN interfaces and the WAN overlay.
  \item The environment supports true routed behavior rather than simple host-to-host tunneling.
\end{itemize}

This setting is required for:
\begin{itemize}
  \item Hub-and-spoke transit traffic.
  \item Spoke-to-spoke communication via the hub.
\end{itemize}

% -----------------------------------------------------------------------------
% 6.5.5 WIREGUARD CRYPTOGRAPHIC POSTURE
%
% Validates encryption and peer authentication model.
%
% Properties:
%   - Modern cryptographic primitives.
%   - Static key-based identity.
%   - Encrypted WAN transport only.
%
% PersistentKeepalive supports NAT traversal and
% failover responsiveness.
% -----------------------------------------------------------------------------
\subsubsection{6.5.5 WireGuard Cryptographic Posture}

\paragraph{Encryption and authentication}
WireGuard provides:
\begin{itemize}
  \item Modern, opinionated cryptography (Curve25519, ChaCha20-Poly1305, BLAKE2s).
  \item Mutual authentication via static public/private key pairs.
  \item Implicit peer authorization through key-based identity.
\end{itemize}

No plaintext traffic traverses the WAN underlay.

\paragraph{PersistentKeepalive usage}
All spoke-to-hub peers are configured with:

\begin{verbatim}
PersistentKeepalive = 25
\end{verbatim}

\paragraph{Operational purpose}
\begin{itemize}
  \item Maintains NAT bindings for spokes behind consumer-grade NAT.
  \item Ensures timely detection of hub reachability changes.
  \item Accelerates failover convergence when the EIP moves between hubs.
\end{itemize}

This setting is especially important given:
\begin{itemize}
  \item The passive adjacency model.
  \item Reliance on public Internet paths with unknown NAT behavior.
\end{itemize}

\paragraph{Key rotation policy}
Key rotation is manual and out of scope for this lab. This is explicitly documented to avoid implying operational maturity beyond the intended proof-of-concept scope.

% -----------------------------------------------------------------------------
% 6.5.6 SECURITY VALIDATION SUMMARY
%
% Confirms:
%   - Encrypted WAN transport.
%   - Restricted public exposure.
%   - Correct forwarding behavior.
%   - Explicit NAT and failover considerations.
%
% Establishes security posture appropriate for a
% small-enterprise WAN proof of concept.
% -----------------------------------------------------------------------------
\subsubsection{6.5.6 Security Validation Summary}

The following security properties are confirmed:
\begin{itemize}
  \item All WAN traffic is encrypted end-to-end using WireGuard.
  \item Hub exposure is tightly constrained by AWS Security Groups.
  \item No unintended packet filtering interferes with routing or convergence.
  \item Kernel settings support routed WAN behavior.
  \item NAT traversal and failover responsiveness are explicitly addressed.
\end{itemize}

Taken together, these observations establish a security posture appropriate for:
\begin{itemize}
  \item A small-enterprise WAN baseline.
  \item A technically honest proof of concept.
  \item A foundation that could be incrementally hardened for production use.
\end{itemize}

% =============================================================================
% SECTION 7: OPERATIONAL CONSIDERATIONS AND RUNBOOK
%
% Purpose:
%   Defines steady-state expectations, validation procedures, and failover
%   behavior for the deployed architecture.
%
% Scope:
%   This section is operationally normative. Procedures described herein
%   reflect expected behavior of a correctly implemented deployment.
%
% Audience:
%   Operators, validation engineers, and maintainers.
% =============================================================================
\section{7. Operational Considerations and Runbook}
\label{sec:operations-runbook}

This section documents the expected operational behavior of the environment, along with a concise runbook intended for engineers responsible for operating, validating, or troubleshooting the deployment. While this lab is a proof of concept, the operational guidance reflects realistic enterprise practices and assumptions.

% -----------------------------------------------------------------------------
% 7.1 NORMAL OPERATIONAL STATE
%
% Defines the canonical steady-state baseline against which deviations
% should be evaluated.
%
% Any divergence from this model requires investigation.
% -----------------------------------------------------------------------------
\subsection{7.1 Normal Operational State}
\label{sec:normal-operational-state}

\paragraph{Steady-state assumptions}
Under normal conditions:
\begin{itemize}
  \item \textbf{DC1-SP1} is in the \texttt{running} state and associated with the Elastic IP (EIP).
  \item \textbf{DC1-SP2} is in the \texttt{running} state but has no EIP association.
  \item All spokes maintain a single WireGuard tunnel (\texttt{wg0}) to the EIP.
  \item BGP sessions between the hub and all spokes are established.
\end{itemize}

From the spokes’ perspective, the hub is a single logical entity reachable via a stable public endpoint and a stable overlay IP (172.16.0.1/32).

\paragraph{Expected dataplane behavior}
\begin{itemize}
  \item Hub-to-spoke and spoke-to-spoke traffic flows exclusively through the hub.
  \item All overlay forwarding decisions are derived from BGP-learned /32 loopback routes installed in the kernel FIB.
  \item No static routes are required for overlay reachability.
\end{itemize}

\subsection{7.2 Startup and Shutdown Order}
\label{sec:startup-shutdown-order}

\paragraph{Recommended startup order}
While the system is resilient to ordering differences, the recommended startup sequence is:
\begin{enumerate}
  \item Start DC1-SP1.
  \item Verify EIP association with DC1-SP1.
  \item Start DC1-SP2.
  \item Start all spokes.
\end{enumerate}

This minimizes transient failover events and ensures predictable initial convergence.

\paragraph{Recommended shutdown order}
For planned maintenance:
\begin{enumerate}
  \item Stop DC1-SP1 (triggers automated failover).
  \item Perform maintenance.
  \item Restart DC1-SP1 (triggers automated failback).
\end{enumerate}

No spoke-side changes are required during planned hub maintenance.

% -----------------------------------------------------------------------------
% 7.3 MANUAL VALIDATION CHECKLIST
%
% These procedures provide deterministic verification of:
%   - Underlay health (WireGuard)
%   - Overlay convergence (BGP)
%   - End-to-end forwarding behavior
%
% Commands are intentionally minimal and rely only on native tooling.
% -----------------------------------------------------------------------------
\subsection{7.3 Manual Validation Checklist}
\label{sec:manual-validation-checklist}

The following checklist can be used at any time to validate operational health.

\subsubsection{7.3.1 WireGuard validation}
\label{sec:wg-validation-runbook}

On the active hub:
\begin{itemize}
  \item Confirm all \texttt{wg} interfaces are up.
  \item Verify recent handshakes for all peers:
\end{itemize}

\begin{verbatim}
sudo wg show
\end{verbatim}

Expected:
\begin{itemize}
  \item Each peer shows a recent handshake (typically < 60 seconds under normal conditions).
  \item Transfer counters increment during traffic tests.
\end{itemize}

\subsubsection{7.3.2 BGP validation}
\label{sec:bgp-validation-runbook}

On the active hub:
\begin{verbatim}
sudo vtysh -c "show ip bgp summary"
\end{verbatim}

Expected:
\begin{itemize}
  \item All spokes in \texttt{Established} state.
  \item Each spoke advertising exactly one /32 loopback.
\end{itemize}

On a spoke:
\begin{verbatim}
sudo vtysh -c "show ip bgp"
\end{verbatim}

Expected:
\begin{itemize}
  \item One route for each remote spoke loopback.
  \item One route for the hub loopback.
\end{itemize}

\subsubsection{7.3.3 End-to-end reachability}
\label{sec:end-to-end-validation-runbook}

From any spoke:
\begin{verbatim}
ping 172.16.0.1
ping 172.16.<other-spoke>.1
\end{verbatim}

Expected:
\begin{itemize}
  \item ICMP replies with stable latency.
  \item TTL values reflect hub traversal for spoke-to-spoke traffic.
\end{itemize}

% -----------------------------------------------------------------------------
% 7.4 FAILOVER OPERATIONS
%
% Describes expected behavior during hub role transitions.
%
% Failover and failback are infrastructure-driven events.
% Routing protocols are not directly aware of redundancy mechanics.
% -----------------------------------------------------------------------------
\subsection{7.4 Failover Operations}
\label{sec:failover-operations}

\subsubsection{7.4.1 Automatic failover triggers}
\label{sec:automatic-failover-triggers}

Failover from DC1-SP1 to DC1-SP2 occurs when either of the following is detected:
\begin{itemize}
  \item EC2 instance state change to \texttt{stopping} or \texttt{stopped} (EventBridge).
  \item CloudWatch alarm indicating failed instance health checks.
\end{itemize}

Both mechanisms invoke the same Lambda failover function.

\subsubsection{7.4.2 Expected failover behavior}
\label{sec:expected-failover-behavior}

\begin{itemize}
  \item The EIP is disassociated from DC1-SP1.
  \item The EIP is associated with DC1-SP2.
  \item WireGuard tunnels re-establish automatically after a brief convergence interval.
  \item BGP sessions re-establish automatically after a brief convergence interval.
\end{itemize}

Transient packet loss during failover is expected and documented.

\subsubsection{7.4.3 Failback behavior}
\label{sec:failback-behavior}

Failback is triggered exclusively by EventBridge when DC1-SP1 transitions to the \texttt{running} state.

\begin{itemize}
  \item The EIP is moved back to DC1-SP1.
  \item DC1-SP2 returns to passive standby.
  \item Overlay connectivity resumes on DC1-SP1 after a brief convergence interval.
\end{itemize}

CloudWatch alarms do not trigger failback, by design.

% -----------------------------------------------------------------------------
% 7.5 OBSERVABILITY AND LOGGING
%
% Establishes authoritative data sources for determining:
%   - Active hub identity
%   - Automation execution status
%   - Convergence confirmation
%
% Operational precedence:
%   1. EIP association state
%   2. Lambda execution logs
%   3. WireGuard/BGP status
% -----------------------------------------------------------------------------
\subsection{7.5 Observability and Logging}
\label{sec:observability-logging}

\paragraph{Primary observability sources}
\begin{itemize}
  \item Lambda execution logs (CloudWatch Logs).
  \item EventBridge rule invocation history.
  \item EC2 instance state transitions.
\end{itemize}

\paragraph{Operational guidance}
\begin{itemize}
  \item Always confirm Lambda invocation before troubleshooting network state.
  \item Use EIP association state as the authoritative indicator of active hub.
  \item Treat WireGuard handshake timestamps as secondary confirmation.
\end{itemize}

\subsection{7.6 Known Limitations}
\label{sec:known-limitations}

\begin{itemize}
  \item Failover is reactive, not predictive.
  \item Packet loss during hub transitions is expected.
  \item Key rotation is manual.
  \item The design does not support active/active hubs.
\end{itemize}

These limitations are acceptable given the lab’s stated scope and objectives.

\subsection{7.7 Operational Summary}
\label{sec:operational-summary}

This runbook demonstrates that the environment:
\begin{itemize}
  \item Is operable without manual reconfiguration during failures.
  \item Exhibits predictable convergence behavior.
  \item Separates control-plane automation from dataplane simplicity.
\end{itemize}

The operational model aligns with enterprise expectations for a small-scale WAN with automated high availability.

% =============================================================================
% SECTION 8: DESIGN TRADE-OFFS AND LESSONS LEARNED
%
% Purpose:
%   Documents architectural decisions, empirical constraints encountered,
%   and intentional trade-offs made during implementation.
%
% Nature of Content:
%   This section is analytical rather than normative. It explains why the
%   final architecture was selected and what alternatives were rejected.
%
% Cross-Reference:
%   Experimental findings referenced here are detailed in Appendix A.
% =============================================================================
\section{8. Design Trade-offs, Lessons Learned, and Future Enhancements}
\label{sec:design-tradeoffs}

This section documents key architectural trade-offs, behavioral constraints, and lessons learned during the design and implementation of the AWS WireGuard hub-and-spoke WAN. It consolidates observations from both the finalized production-oriented design and the exploratory constraints described in Appendix~A, with a focus on decisions that materially affected correctness, operability, and realism.

% -----------------------------------------------------------------------------
% 8.1 UNDERLAY VS OVERLAY SEPARATION
%
% Core architectural validation:
%   WireGuard enforces transport-level cryptographic constraints that
%   supersede routing protocol logic.
%
% This separation is a foundational outcome of the lab.
% -----------------------------------------------------------------------------
\subsection{8.1 Underlay vs. Overlay Separation}
\label{subsec:underlay-overlay-separation}

A critical architectural principle validated by this lab is the strict separation between the underlay and overlay planes.

\subsubsection{8.1.1 WireGuard as the Underlay}
\label{subsubsec:wireguard-underlay}

WireGuard tunnels serve as the underlay transport layer. Their responsibilities are limited to:
\begin{itemize}
\item Cryptographic security (encryption and authentication)
\item Point-to-point IP reachability
\item Static peer-based forwarding via \texttt{AllowedIPs}
\end{itemize}

WireGuard does not perform routing decisions and does not dynamically resolve reachability. As demonstrated during the active/active hub experiment documented in Appendix~A, cryptographic constraints imposed by \texttt{AllowedIPs} override all routing protocol logic. When a destination prefix is not covered by a valid WireGuard peer association, traffic fails with errors such as:

\begin{verbatim}
ping: sendmsg: Required key not available
\end{verbatim}

This behavior is fundamental to WireGuard’s design and must be treated as an underlay constraint rather than a routing failure.

\subsubsection{8.1.2 BGP as the Overlay}
\label{subsubsec:bgp-overlay}

BGP operates strictly as the overlay control plane. Its role is to:
\begin{itemize}
\item Advertise loopback reachability
\item Enable spoke-to-spoke routing via the hub
\item Abstract physical and transport-level topology
\end{itemize}

The overlay is entirely dependent on underlay correctness. Valid BGP routes do not guarantee reachability unless the underlying WireGuard peer configuration permits encrypted transport. This dependency was explicitly observed during attempts to form parallel adjacencies over overlapping WireGuard tunnels, as detailed in Appendix~A.

\subsection{8.2 Active/Standby vs. Active/Active Hub Design}
\label{subsec:active-standby-vs-active-active}

% -----------------------------------------------------------------------------
% 8.2.1 ACTIVE/ACTIVE HUB ATTEMPT
%
% This subsection documents an experimental design that was rejected.
% Failures observed were due to protocol properties, not misconfiguration.
%
% Included to preserve institutional knowledge and prevent rework.
% -----------------------------------------------------------------------------
\subsubsection{8.2.1 Active/Active Hub Attempt}
\label{subsubsec:active-active-attempt}

An initial design goal was to implement an active/active hub model in which spokes maintained simultaneous WireGuard tunnels and BGP adjacencies to both hubs.

This approach empirically encountered several non-obvious constraints:
\begin{itemize}
\item Overlapping \texttt{AllowedIPs} entries caused kernel route collisions
\item WireGuard interface instantiation failed due to duplicate route insertion
\item Traffic forwarding failed despite valid BGP routes
\item Cryptographic enforcement prevented asymmetric return paths
\end{itemize}

These issues are documented in detail in Appendix~A and are intrinsic to WireGuard’s security model rather than misconfiguration errors.

% -----------------------------------------------------------------------------
% 8.2.2 ACTIVE/STANDBY MODEL SELECTION
%
% Defines the final architectural decision adopted in Sections 4–7.
%
% Emphasizes determinism and cryptographic correctness over load-sharing.
% -----------------------------------------------------------------------------
\subsubsection{8.2.2 Active/Standby Hub Model Selection}
\label{subsubsec:active-standby-selection}

The final design intentionally adopts an active/standby hub model:
\begin{itemize}
\item Only one hub is reachable at any given time via the Elastic IP
\item Spokes maintain a single WireGuard tunnel
\item BGP adjacency is unambiguous and deterministic
\item Failover is handled externally via AWS automation
\end{itemize}

This approach trades instantaneous load-sharing for:
\begin{itemize}
\item Predictable routing behavior
\item Cryptographic correctness
\item Operational simplicity
\end{itemize}

These properties are operationally validated in Section~\ref{sec:connectivity-validation} through traceroute and failover testing.

\subsection{8.3 Routing Protocol Selection: BGP vs. OSPF}
\label{subsec:routing-protocol-selection}

\subsubsection{8.3.1 OSPF Limitations over WireGuard}
\label{subsubsec:ospf-limitations}

Standard OSPF relies on multicast for neighbor discovery and adjacency formation, which is incompatible with WireGuard’s strictly unicast design. While OSPF NBMA mode was considered, it introduces:
\begin{itemize}
\item Manual neighbor configuration
\item Increased complexity without clear operational benefit
\item Reduced alignment with real-world WAN deployments
\end{itemize}

\subsubsection{8.3.2 BGP Selection Rationale}
\label{subsubsec:bgp-selection-rationale}

BGP was selected due to:
\begin{itemize}
\item Native unicast operation
\item Explicit peer configuration
\item Strong alignment with enterprise WAN and SD-WAN architectures
\item Better suitability for hub-and-spoke topologies
\end{itemize}

The choice also intentionally expanded the lab beyond entry-level routing protocols, reinforcing production-oriented design thinking.

% -----------------------------------------------------------------------------
% 8.4 BGP POLICY SIMPLIFICATION
%
% Explicitly documents deviation from production best practices for the
% sake of instructional clarity.
%
% Prevents misinterpretation of this lab configuration as hardened policy.
% -----------------------------------------------------------------------------
\subsection{8.4 BGP Policy Simplification}
\label{subsec:bgp-policy-simplification}

\subsubsection{8.4.1 Use of \texttt{no bgp ebgp-requires-policy}}
\label{subsubsec:no-ebgp-requires-policy}

The configuration explicitly disables FRR’s default requirement for inbound and outbound eBGP policy:

\begin{verbatim}
no bgp ebgp-requires-policy
\end{verbatim}

This decision was intentional and scoped to the lab environment:
\begin{itemize}
\item All BGP sessions are tightly controlled point-to-point links
\item Advertised prefixes are limited to loopback /32s
\item There is no realistic risk of route leakage within the lab’s controlled scope
\end{itemize}

\subsubsection{8.4.2 Production Implications}
\label{subsubsec:bgp-production-implications}

In a production deployment, explicit routing policy would be mandatory:
\begin{itemize}
\item Prefix-lists to restrict advertisements
\item Route-maps to enforce directionality
\item Explicit filtering to prevent unintended propagation
\end{itemize}

This omission represents a conscious trade-off between instructional clarity and production hardening.

\subsection{8.5 Automation vs. Routing-Based Failover}
\label{subsec:automation-vs-routing-failover}

Rather than relying on routing protocol convergence to detect hub failure, the design delegates hub availability to AWS-native automation:
\begin{itemize}
\item EventBridge rules for instance lifecycle events
\item CloudWatch alarms for health-based failure detection
\item Lambda functions for deterministic Elastic IP reassociation
\end{itemize}

This approach is intended to provide:
\begin{itemize}
\item Faster failover than routing timers alone
\item Clear separation of infrastructure availability and routing logic
\item Predictable behavior under partial failure conditions
\end{itemize}

These properties are empirically visible in the bounded packet-loss windows and convergence behavior measured in Section~\ref{sec:connectivity-validation}.

\subsection{8.6 Future Enhancements}
\label{subsec:future-enhancements}

Potential future enhancements include:
\begin{itemize}
\item Active/active hubs using non-overlapping WireGuard tunnels and per-spoke VRFs
\item Explicit BGP policy enforcement
\item Multiple regional hubs with hierarchical routing
\item Automated key rotation and peer provisioning
\item Integration with managed SD-WAN or transit gateway architectures
\end{itemize}

Each enhancement would preserve the underlay/overlay separation validated by this lab while scaling the design toward larger enterprise deployments.

% =============================================================================
% SECTION 9: CONCLUSION AND NEXT STEPS
%
% Purpose:
%   Summarizes validated outcomes, confirms objective alignment, and
%   outlines potential evolution paths beyond the lab scope.
%
% Nature:
%   This section is summative and forward-looking rather than normative.
% =============================================================================
\section{9. Conclusion and Next Steps}
\label{sec:conclusion-next-steps}

This document has presented the design, implementation, validation, and operational considerations of an AWS-based WireGuard WAN employing a hub-and-spoke topology with automated active/standby high availability. Although implemented as a proof-of-concept lab, the architecture intentionally mirrors small-enterprise design principles and operational workflows validated throughout Section~\ref{sec:validation-testing} and Section~\ref{sec:connectivity-validation}.

\subsection{9.1 Summary of Outcomes}
\label{sec:conclusion-summary-outcomes}

The lab successfully demonstrates the following:

\begin{itemize}
  \item A functional hub-and-spoke WAN using WireGuard as the encrypted underlay (Section~\ref{sec:wg-underlay-validation}).
  \item Dynamic routing via eBGP between the active hub and all spokes, eliminating static routing dependencies (Section~\ref{sec:bgp-overlay-validation}).
  \item Automated hub failover and failback using AWS-native event-driven automation (Section~\ref{sec:connectivity-validation}).
  \item Preservation of routing correctness and reachability after a bounded convergence window during hub transitions.
\end{itemize}

From a network perspective, all spokes maintained steady-state reachability to:
\begin{itemize}
  \item The hub loopback address (\texttt{172.16.0.1/32}).
  \item All other spoke loopback addresses.
\end{itemize}

From a cloud infrastructure perspective, Elastic IP reassociation was validated as a deterministic mechanism for defining hub ownership in this lab environment, with correctness and behavior explicitly demonstrated during failover and failback testing in Section~\ref{sec:connectivity-validation}.

\subsection{9.2 Validation of Design Objectives}
\label{sec:conclusion-objectives}

The original objectives of the lab were met:

\begin{enumerate}
  \item \textbf{Enterprise realism:}  
  Design decisions align with patterns appropriate for a small-enterprise WAN proof of concept, while explicitly documenting areas not hardened for full production use (Sections~\ref{sec:security-validation} and \ref{sec:design-tradeoffs}).

  \item \textbf{Automation-first HA:}  
  High availability was implemented without reliance on traditional L2/L3 redundancy protocols, instead leveraging AWS-native event-driven automation for hub ownership and availability (Section~\ref{sec:ha-automation-aws}).

  \item \textbf{Operational clarity:}  
  Failover behavior is observable, auditable, and repeatable using standard AWS tooling and deterministic Elastic IP association (Sections~\ref{sec:ha-automation-aws} and \ref{sec:operations-runbook}).

  \item \textbf{Simplicity at the edge:}  
  Spokes remain single-homed with no hub-selection logic, maintaining a single WireGuard tunnel and eBGP adjacency toward the active hub while relying on AWS automation for hub identity changes.
\end{enumerate}

\subsection{9.3 Practical Implications}
\label{sec:conclusion-implications}

This design highlights several practical takeaways relevant to real-world deployments:

\begin{itemize}
  \item Cloud networking constraints often require rethinking traditional HA patterns.
  \item Deterministic ownership models (for example, Elastic IPs) simplify failure reasoning and convergence behavior.
  \item Event-driven automation can replace protocol-driven redundancy when properly scoped to the architecture and operational context.
\end{itemize}

The lab also underscores the importance of validating assumptions empirically, particularly when integrating routing protocols with cloud automation and infrastructure state.

\subsection{9.4 Recommended Next Steps}
\label{sec:conclusion-next-steps-actions}

If this architecture were to be evolved beyond a lab environment, the following steps are recommended:

\begin{itemize}
  \item Introduce staged environments (dev / test / prod) to validate change workflows.
  \item Add monitoring and alerting around BGP session health and WireGuard tunnel stability.
  \item Formalize key management and rotation procedures.
  \item Evaluate whether active/active hubs are warranted based on throughput and availability requirements.
\end{itemize}

\subsection{9.5 Closing Remarks}
\label{sec:conclusion-closing}

While intentionally minimal in scope, this lab demonstrates a coherent and defensible approach to building a secure, automated WAN in AWS using open-source networking tools. The resulting architecture serves both as a validation of design principles and as a foundation upon which more complex enterprise deployments could be built.


% =============================================================================
% APPENDIX A: DESIGN CONSTRAINTS AND LIMITATIONS
%
% Purpose:
%   Documents protocol-level constraints, observed failure modes, and
%   evaluated but rejected design approaches encountered during iterative
%   validation of the AWS WireGuard BGP hub-and-spoke WAN.
%
% Architectural Role:
%   Preserves empirical findings and non-obvious behavioral constraints
%   that materially influenced the final architecture but are not part of
%   the normative configuration model defined in Sections 5 and 6.
%
%   This appendix clarifies why certain alternatives (e.g., OSPF overlay,
%   active/active dual hubs) were not adopted despite theoretical viability.
%
% Change Sensitivity:
%   Modifications to assumptions documented here may require reevaluation
%   of routing policy, cryptographic ownership boundaries, interface design,
%   and failover mechanics. Review Section 5 (Detailed Design) and
%   Section 6 (High Availability) before altering architectural constraints.
% =============================================================================
\appendix
\section{Appendix: Design Constraints and Limitations}
\label{sec:appendix-design-constraints}

This appendix documents protocol-level constraints and architectural limitations encountered during the design and validation of the AWS WireGuard BGP hub-and-spoke WAN. The purpose is to preserve clarity in the primary design document while explicitly capturing evaluated alternatives, rejected approaches, and empirically observed behaviors that materially influenced the final architecture.

The observations in this appendix were derived from live testing, Linux kernel routing behavior, WireGuard peer-selection enforcement, and routing protocol control-plane validation during iterative design attempts (including an active/active hub trial that was ultimately rejected in favor of active/standby).

\subsection{A.1 Routing Protocol Selection: BGP versus OSPF over WireGuard}
\label{subsec:a1-routing-protocol-selection}

\subsubsection{A.1.1 Problem Context}
\label{subsubsec:a1-problem-context}

OSPF was initially considered as a candidate routing protocol for dynamic spoke-to-hub and spoke-to-spoke reachability. In enterprise environments, OSPF is a common interior gateway protocol (IGP) with fast convergence characteristics and straightforward operational tooling.

However, baseline OSPF adjacency formation depends on multicast packets (notably to 224.0.0.5 and 224.0.0.6) for neighbor discovery and database synchronization. WireGuard tunnels are point-to-point and strictly unicast-oriented; they do not provide L2 broadcast domains and do not natively support multicast semantics in the manner expected by default OSPF deployments.

As a result, standard OSPF configurations are not a clean fit for a routed overlay built on WireGuard tunnels without adopting additional design constraints or workarounds.

\subsubsection{A.1.2 OSPF NBMA Consideration}
\label{subsubsec:a1-ospf-nbma}

OSPF NBMA (Non-Broadcast Multi-Access) mode was considered as a workaround. In NBMA mode, multicast neighbor discovery is replaced by manually defined neighbor relationships, which can make OSPF functional across topologies that do not support broadcast/multicast behavior (historically, technologies such as Frame Relay).

While OSPF NBMA can be made to operate over unicast-only transport, it introduces additional operational complexity:
\begin{itemize}
  \item Manual neighbor definitions and lifecycle management.
  \item Increased sensitivity to topology changes and configuration drift.
  \item Reduced operational simplicity relative to typical enterprise OSPF LAN usage.
\end{itemize}

Given that the lab was intentionally structured as a WAN-like hub-and-spoke overlay (rather than a single-site IGP inside a broadcast domain), OSPF NBMA was treated as feasible but not optimal.

\subsubsection{A.1.3 Rationale for BGP Selection}
\label{subsubsec:a1-bgp-rationale}

BGP was selected for the following reasons:
\begin{itemize}
  \item Native unicast operation aligns cleanly with WireGuard underlay transport characteristics.
  \item Explicit peering configuration matches the hub-and-spoke overlay model and avoids reliance on multicast discovery.
  \item BGP’s policy and topology flexibility more closely resembles common enterprise WAN and SD-WAN control-plane patterns.
  \item Use of distinct AS numbers per spoke provides realistic separation of routing domains and supports hub-and-spoke WAN conventions.
\end{itemize}

In addition, BGP was selected as an intentional design choice to reflect production-oriented WAN behavior rather than campus IGP assumptions.

\textbf{Conclusion:} In this lab, BGP proved to be the most appropriate overlay control-plane protocol for a secure, routed, hub-and-spoke architecture built on a unicast-only WireGuard underlay.

% -----------------------------------------------------------------------------
% NOTE: ALLOWEDIPS AS A FORWARDING AUTHORITY
%
% WireGuard's AllowedIPs parameter functions simultaneously as:
%   1. A cryptographic authorization boundary
%   2. A peer selection mechanism
%   3. An implicit forwarding constraint
%
% Control-plane routing state (e.g., BGP-installed routes) does not
% override cryptographic ownership. If a destination prefix is not
% covered by AllowedIPs, traffic will be rejected prior to encryption,
% even when kernel routing appears correct.
%
% Design Implication:
%   Cryptographic prefix ownership must be validated before evaluating
%   routing protocol correctness. Overlay routing cannot compensate for
%   incomplete or overlapping AllowedIPs definitions.
% -----------------------------------------------------------------------------
\subsection{A.2 WireGuard \texttt{AllowedIPs} as a Cryptographic Routing Constraint}
\label{subsec:a2-allowedips-constraint}

\subsubsection{A.2.1 Observed Failure Mode}
\label{subsubsec:a2-observed-failure}

During iterative design attempts (including early active/active trials), insufficient or overly restrictive \texttt{AllowedIPs} definitions caused data-plane failures even when the control-plane (BGP) appeared healthy.

A representative failure observed during spoke-to-spoke testing is shown below:

\begin{verbatim}
PING 172.16.1.1 (172.16.1.1) 56(84) bytes of data.
From 10.20.30.12 icmp_seq=1 Destination Host Unreachable
ping: sendmsg: Required key not available
From 10.20.30.12 icmp_seq=2 Destination Host Unreachable
ping: sendmsg: Required key not available
From 10.20.30.12 icmp_seq=3 Destination Host Unreachable
ping: sendmsg: Required key not available
\end{verbatim}

In the same misconfigured state, spokes could reach hubs, and hubs could reach spokes, but spokes could not reliably reach each other. This mismatch between control-plane routing visibility and data-plane delivery was a recurring diagnostic indicator.

\subsubsection{A.2.2 Interpretation}
\label{subsubsec:a2-interpretation}

This error occurs when a packet is routed toward a WireGuard interface, but WireGuard cannot select a peer/key because the destination IP does not fall within any peer’s configured \texttt{AllowedIPs}. In effect, WireGuard rejects the packet before encryption because there is no authorized cryptographic mapping for that destination address.

Practically, this implies the following:
\begin{itemize}
  \item Kernel routing state can correctly indicate a next hop (e.g., installed via BGP).
  \item The packet can be forwarded to a WireGuard interface.
  \item WireGuard can still drop the packet if the destination prefix is not covered by \texttt{AllowedIPs}.
\end{itemize}

\subsubsection{A.2.3 Architectural Implication}
\label{subsubsec:a2-architectural-implication}

This behavior demonstrates that, in this lab, \texttt{AllowedIPs} functions simultaneously as:
\begin{itemize}
  \item A cryptographic access-control list defining what traffic is permitted to be encrypted/decrypted.
  \item A peer-selection mechanism defining which peer owns a destination prefix.
  \item A hard data-plane forwarding constraint that routing protocols cannot override.
\end{itemize}

\textbf{Conclusion:} In WireGuard-based routed overlays, cryptographic policy is authoritative. Overlay routing protocols cannot compensate for missing or overlapping \texttt{AllowedIPs} coverage.

% -----------------------------------------------------------------------------
% A.3 ACTIVE/ACTIVE HUB DESIGN CONSTRAINTS
%
% A true active/active design requires deterministic separation of:
%   - Route installation
%   - Prefix ownership
%   - Peer selection logic
%
% In this lab environment, wg-quick automatic route injection and shared
% routing tables produced prefix collisions when overlapping destination
% space was introduced across parallel tunnels.
%
% Absent VRF isolation, policy-based routing, or fwmark-based table
% separation, deterministic multipath behavior could not be achieved.
%
% Design Outcome:
%   Active/standby failover was selected to preserve deterministic
%   prefix ownership and predictable operational behavior.
% -----------------------------------------------------------------------------
\subsection{A.3 Active/Active Hub Design Constraints}
\label{subsec:a3-active-active-constraints}

\subsubsection{A.3.1 Active/Active Design Hypothesis}
\label{subsubsec:a3-hypothesis}

An early design goal was a true active/active dual-hub topology in which each spoke maintained two concurrent WireGuard tunnels (one to each hub), and BGP multipath would provide redundancy and potential load sharing.

This would more closely resemble advanced WAN designs, but it imposes strict requirements on tunnel separation, routing policy, and cryptographic prefix ownership.

\subsubsection{A.3.2 Interface and Route Collision Evidence}
\label{subsubsec:a3-collision-evidence}

When attempting to bring up a second WireGuard interface on a spoke (\texttt{wg1}) with overlapping destination coverage, interface initialization failed when \texttt{wg-quick} attempted to add routes that already existed via another interface:

\begin{verbatim}
[#] ip link add wg1 type wireguard
[#] wg setconf wg1 /dev/fd/63
[#] ip -4 address add 10.110.1.2/30 dev wg1
[#] ip link set mtu 1420 up dev wg1
[#] ip -4 route add 172.16.0.0/16 dev wg1
RTNETLINK answers: File exists
[#] ip link delete dev wg1
\end{verbatim}

After the failure, the interface did not persist:

\begin{verbatim}
ip addr show wg1
Device "wg1" does not exist.
\end{verbatim}

\subsubsection{A.3.3 Root Cause and General Constraint}
\label{subsubsec:a3-root-cause}

The immediate failure was due to \texttt{wg-quick}'s automatic route injection colliding with existing routes. More fundamentally, the active/active approach created overlapping address ownership expectations across tunnels:

\begin{itemize}
  \item If both \texttt{wg0} and \texttt{wg1} claim reachability to the same destination space (e.g., 172.16.0.0/16), kernel route insertion conflicts occur.
  \item Even if route installation is manually suppressed, WireGuard still requires deterministic peer selection for each destination prefix; overlapping \texttt{AllowedIPs} ownership undermines that determinism without additional separation mechanisms.
\end{itemize}

\subsubsection{A.3.4 Architectural Consequence}
\label{subsubsec:a3-architectural-consequence}

In this lab, a true active/active dual-hub design would require constructs beyond basic \texttt{wg-quick} defaults, such as:
\begin{itemize}
  \item VRF separation per tunnel.
  \item Policy-based routing or \texttt{fwmark}-based routing tables.
  \item Non-overlapping prefix partitioning per hub.
  \item Higher-layer SD-WAN abstractions for intent-based forwarding.
\end{itemize}

This lab intentionally prioritized a design that remains operationally realistic and debuggable without introducing VRFs, policy routing, or complex per-prefix tunnel segmentation.

\textbf{Conclusion:} In the observed lab environment, WireGuard favors deterministic, single-owner forwarding models. Active/standby hub failover aligns naturally with these constraints and was therefore selected for the final design.

\subsection{A.4 Implications for Enterprise WAN Design}
\label{subsec:a4-enterprise-implications}

The constraints documented in this appendix directly informed the final architectural decisions:
\begin{itemize}
  \item BGP was selected as the overlay control-plane protocol due to unicast-native behavior and enterprise WAN alignment.
  \item Active/standby hub failover was selected to avoid overlapping cryptographic prefix ownership and \texttt{wg-quick} route collisions.
  \item \texttt{AllowedIPs} coverage was treated as a first-order design requirement, enforced prior to (and independent of) routing protocol correctness.
\end{itemize}

These choices yielded a deterministic, auditable, enterprise-aligned WAN architecture with automated failover/failback and validated spoke-to-spoke reachability.

\subsection{A.5 Summary}
\label{subsec:a5-summary}

Appendix A captures design constraints that were foundational rather than incidental. By explicitly recording protocol limitations, observed failure modes, and rejected design approaches, the final design remains technically defensible and provides a clear baseline for future enhancements (e.g., VRFs and policy routing) if active/active operation is later required.

\bigskip

\noindent\textit{End of document.}

\end{document}
